{"ast":null,"code":"import { InMemoryCache } from \"../cache/index.js\";\nimport { AIMessage, GenerationChunk, RUN_KEY } from \"../schema/index.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\nimport { getBufferString } from \"../memory/base.js\";\n/**\n * LLM Wrapper. Provides an {@link call} (an {@link generate}) function that takes in a prompt (or prompts) and returns a string.\n */\nexport class BaseLLM extends BaseLanguageModel {\n  constructor({\n    cache,\n    concurrency,\n    ...rest\n  }) {\n    super(concurrency ? {\n      maxConcurrency: concurrency,\n      ...rest\n    } : rest);\n    Object.defineProperty(this, \"lc_namespace\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: [\"langchain\", \"llms\", this._llmType()]\n    });\n    Object.defineProperty(this, \"cache\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    if (typeof cache === \"object\") {\n      this.cache = cache;\n    } else if (cache) {\n      this.cache = InMemoryCache.global();\n    } else {\n      this.cache = undefined;\n    }\n  }\n  /**\n   * This method takes an input and options, and returns a string. It\n   * converts the input to a prompt value and generates a result based on\n   * the prompt.\n   * @param input Input for the LLM.\n   * @param options Options for the LLM call.\n   * @returns A string result based on the prompt.\n   */\n  async invoke(input, options) {\n    const promptValue = BaseLLM._convertInputToPromptValue(input);\n    const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n    return result.generations[0][0].text;\n  }\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(_input, _options, _runManager) {\n    throw new Error(\"Not implemented.\");\n  }\n  _separateRunnableConfigFromCallOptions(options) {\n    const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n    if (callOptions?.timeout && !callOptions.signal) {\n      callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n    }\n    return [runnableConfig, callOptions];\n  }\n  async *_streamIterator(input, options) {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (this._streamResponseChunks === BaseLLM.prototype._streamResponseChunks) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseLLM._convertInputToPromptValue(input);\n      const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n      const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions)\n      };\n      const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), [prompt.toString()], undefined, undefined, extra);\n      let generation = new GenerationChunk({\n        text: \"\"\n      });\n      try {\n        for await (const chunk of this._streamResponseChunks(input.toString(), callOptions, runManagers?.[0])) {\n          if (!generation) {\n            generation = chunk;\n          } else {\n            generation = generation.concat(chunk);\n          }\n          if (typeof chunk.text === \"string\") {\n            yield chunk.text;\n          }\n        }\n      } catch (err) {\n        await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMError(err)));\n        throw err;\n      }\n      await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMEnd({\n        generations: [[generation]]\n      })));\n    }\n  }\n  /**\n   * This method takes prompt values, options, and callbacks, and generates\n   * a result based on the prompts.\n   * @param promptValues Prompt values for the LLM.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns An LLMResult based on the prompts.\n   */\n  async generatePrompt(promptValues, options, callbacks) {\n    const prompts = promptValues.map(promptValue => promptValue.toString());\n    return this.generate(prompts, options, callbacks);\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options) {\n    return {};\n  }\n  _flattenLLMResult(llmResult) {\n    const llmResults = [];\n    for (let i = 0; i < llmResult.generations.length; i += 1) {\n      const genList = llmResult.generations[i];\n      if (i === 0) {\n        llmResults.push({\n          generations: [genList],\n          llmOutput: llmResult.llmOutput\n        });\n      } else {\n        const llmOutput = llmResult.llmOutput ? {\n          ...llmResult.llmOutput,\n          tokenUsage: {}\n        } : undefined;\n        llmResults.push({\n          generations: [genList],\n          llmOutput\n        });\n      }\n    }\n    return llmResults;\n  }\n  /** @ignore */\n  async _generateUncached(prompts, parsedOptions, handledOptions) {\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions)\n    };\n    const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, undefined, undefined, extra);\n    let output;\n    try {\n      output = await this._generate(prompts, parsedOptions, runManagers?.[0]);\n    } catch (err) {\n      await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMError(err)));\n      throw err;\n    }\n    const flattenedOutputs = this._flattenLLMResult(output);\n    await Promise.all((runManagers ?? []).map((runManager, i) => runManager?.handleLLMEnd(flattenedOutputs[i])));\n    const runIds = runManagers?.map(manager => manager.runId) || undefined;\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runIds ? {\n        runIds\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  /**\n   * Run the LLM on the given prompts and input, handling caching.\n   */\n  async generate(prompts, options, callbacks) {\n    if (!Array.isArray(prompts)) {\n      throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n    }\n    let parsedOptions;\n    if (Array.isArray(options)) {\n      parsedOptions = {\n        stop: options\n      };\n    } else {\n      parsedOptions = options;\n    }\n    const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n    runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n    if (!this.cache) {\n      return this._generateUncached(prompts, callOptions, runnableConfig);\n    }\n    const {\n      cache\n    } = this;\n    const params = this.serialize();\n    params.stop = callOptions.stop ?? params.stop;\n    const llmStringKey = `${Object.entries(params).sort()}`;\n    const missingPromptIndices = [];\n    const generations = await Promise.all(prompts.map(async (prompt, index) => {\n      const result = await cache.lookup(prompt, llmStringKey);\n      if (!result) {\n        missingPromptIndices.push(index);\n      }\n      return result;\n    }));\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(missingPromptIndices.map(i => prompts[i]), callOptions, runnableConfig);\n      await Promise.all(results.generations.map(async (generation, index) => {\n        const promptIndex = missingPromptIndices[index];\n        generations[promptIndex] = generation;\n        return cache.update(prompts[promptIndex], llmStringKey, generation);\n      }));\n      llmOutput = results.llmOutput ?? {};\n    }\n    return {\n      generations,\n      llmOutput\n    };\n  }\n  /**\n   * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n   */\n  async call(prompt, options, callbacks) {\n    const {\n      generations\n    } = await this.generate([prompt], options, callbacks);\n    return generations[0][0].text;\n  }\n  /**\n   * This method is similar to `call`, but it's used for making predictions\n   * based on the input text.\n   * @param text Input text for the prediction.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns A prediction based on the input text.\n   */\n  async predict(text, options, callbacks) {\n    return this.call(text, options, callbacks);\n  }\n  /**\n   * This method takes a list of messages, options, and callbacks, and\n   * returns a predicted message.\n   * @param messages A list of messages for the prediction.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns A predicted message based on the list of messages.\n   */\n  async predictMessages(messages, options, callbacks) {\n    const text = getBufferString(messages);\n    const prediction = await this.call(text, options, callbacks);\n    return new AIMessage(prediction);\n  }\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams() {\n    return {};\n  }\n  /**\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this._identifyingParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  _modelType() {\n    return \"base_llm\";\n  }\n  /**\n   * Load an LLM from a json-like object describing it.\n   */\n  static async deserialize(data) {\n    const {\n      _type,\n      _model,\n      ...rest\n    } = data;\n    if (_model && _model !== \"base_llm\") {\n      throw new Error(`Cannot load LLM with model ${_model}`);\n    }\n    const Cls = {\n      openai: (await import(\"./openai.js\")).OpenAI\n    }[_type];\n    if (Cls === undefined) {\n      throw new Error(`Cannot load  LLM with type ${_type}`);\n    }\n    return new Cls(rest);\n  }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport class LLM extends BaseLLM {\n  async _generate(prompts, options, runManager) {\n    const generations = await Promise.all(prompts.map((prompt, promptIndex) => this._call(prompt, {\n      ...options,\n      promptIndex\n    }, runManager).then(text => [{\n      text\n    }])));\n    return {\n      generations\n    };\n  }\n}","map":{"version":3,"names":["InMemoryCache","AIMessage","GenerationChunk","RUN_KEY","BaseLanguageModel","CallbackManager","getBufferString","BaseLLM","constructor","cache","concurrency","rest","maxConcurrency","Object","defineProperty","enumerable","configurable","writable","value","_llmType","global","undefined","invoke","input","options","promptValue","_convertInputToPromptValue","result","generatePrompt","callbacks","generations","text","_streamResponseChunks","_input","_options","_runManager","Error","_separateRunnableConfigFromCallOptions","runnableConfig","callOptions","timeout","signal","AbortSignal","_streamIterator","prototype","prompt","callbackManager_","configure","tags","metadata","verbose","extra","invocation_params","invocationParams","runManagers","handleLLMStart","toJSON","toString","generation","chunk","concat","err","Promise","all","map","runManager","handleLLMError","handleLLMEnd","promptValues","prompts","generate","_flattenLLMResult","llmResult","llmResults","i","length","genList","push","llmOutput","tokenUsage","_generateUncached","parsedOptions","handledOptions","output","_generate","flattenedOutputs","runIds","manager","runId","Array","isArray","stop","params","serialize","llmStringKey","entries","sort","missingPromptIndices","index","lookup","results","promptIndex","update","call","predict","predictMessages","messages","prediction","_identifyingParams","_type","_model","_modelType","deserialize","data","Cls","openai","OpenAI","LLM","_call","then"],"sources":["/Users/mayamagavi/instalily/case-study/node_modules/langchain/dist/llms/base.js"],"sourcesContent":["import { InMemoryCache } from \"../cache/index.js\";\nimport { AIMessage, GenerationChunk, RUN_KEY, } from \"../schema/index.js\";\nimport { BaseLanguageModel, } from \"../base_language/index.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\nimport { getBufferString } from \"../memory/base.js\";\n/**\n * LLM Wrapper. Provides an {@link call} (an {@link generate}) function that takes in a prompt (or prompts) and returns a string.\n */\nexport class BaseLLM extends BaseLanguageModel {\n    constructor({ cache, concurrency, ...rest }) {\n        super(concurrency ? { maxConcurrency: concurrency, ...rest } : rest);\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"llms\", this._llmType()]\n        });\n        Object.defineProperty(this, \"cache\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        if (typeof cache === \"object\") {\n            this.cache = cache;\n        }\n        else if (cache) {\n            this.cache = InMemoryCache.global();\n        }\n        else {\n            this.cache = undefined;\n        }\n    }\n    /**\n     * This method takes an input and options, and returns a string. It\n     * converts the input to a prompt value and generates a result based on\n     * the prompt.\n     * @param input Input for the LLM.\n     * @param options Options for the LLM call.\n     * @returns A string result based on the prompt.\n     */\n    async invoke(input, options) {\n        const promptValue = BaseLLM._convertInputToPromptValue(input);\n        const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n        return result.generations[0][0].text;\n    }\n    // eslint-disable-next-line require-yield\n    async *_streamResponseChunks(_input, _options, _runManager) {\n        throw new Error(\"Not implemented.\");\n    }\n    _separateRunnableConfigFromCallOptions(options) {\n        const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n        if (callOptions?.timeout && !callOptions.signal) {\n            callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n        }\n        return [runnableConfig, callOptions];\n    }\n    async *_streamIterator(input, options) {\n        // Subclass check required to avoid double callbacks with default implementation\n        if (this._streamResponseChunks === BaseLLM.prototype._streamResponseChunks) {\n            yield this.invoke(input, options);\n        }\n        else {\n            const prompt = BaseLLM._convertInputToPromptValue(input);\n            const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n            const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: callOptions,\n                invocation_params: this?.invocationParams(callOptions),\n            };\n            const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), [prompt.toString()], undefined, undefined, extra);\n            let generation = new GenerationChunk({\n                text: \"\",\n            });\n            try {\n                for await (const chunk of this._streamResponseChunks(input.toString(), callOptions, runManagers?.[0])) {\n                    if (!generation) {\n                        generation = chunk;\n                    }\n                    else {\n                        generation = generation.concat(chunk);\n                    }\n                    if (typeof chunk.text === \"string\") {\n                        yield chunk.text;\n                    }\n                }\n            }\n            catch (err) {\n                await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n                throw err;\n            }\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMEnd({\n                generations: [[generation]],\n            })));\n        }\n    }\n    /**\n     * This method takes prompt values, options, and callbacks, and generates\n     * a result based on the prompts.\n     * @param promptValues Prompt values for the LLM.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns An LLMResult based on the prompts.\n     */\n    async generatePrompt(promptValues, options, callbacks) {\n        const prompts = promptValues.map((promptValue) => promptValue.toString());\n        return this.generate(prompts, options, callbacks);\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams(_options) {\n        return {};\n    }\n    _flattenLLMResult(llmResult) {\n        const llmResults = [];\n        for (let i = 0; i < llmResult.generations.length; i += 1) {\n            const genList = llmResult.generations[i];\n            if (i === 0) {\n                llmResults.push({\n                    generations: [genList],\n                    llmOutput: llmResult.llmOutput,\n                });\n            }\n            else {\n                const llmOutput = llmResult.llmOutput\n                    ? { ...llmResult.llmOutput, tokenUsage: {} }\n                    : undefined;\n                llmResults.push({\n                    generations: [genList],\n                    llmOutput,\n                });\n            }\n        }\n        return llmResults;\n    }\n    /** @ignore */\n    async _generateUncached(prompts, parsedOptions, handledOptions) {\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n        };\n        const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, undefined, undefined, extra);\n        let output;\n        try {\n            output = await this._generate(prompts, parsedOptions, runManagers?.[0]);\n        }\n        catch (err) {\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n            throw err;\n        }\n        const flattenedOutputs = this._flattenLLMResult(output);\n        await Promise.all((runManagers ?? []).map((runManager, i) => runManager?.handleLLMEnd(flattenedOutputs[i])));\n        const runIds = runManagers?.map((manager) => manager.runId) || undefined;\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, RUN_KEY, {\n            value: runIds ? { runIds } : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Run the LLM on the given prompts and input, handling caching.\n     */\n    async generate(prompts, options, callbacks) {\n        if (!Array.isArray(prompts)) {\n            throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n        }\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else {\n            parsedOptions = options;\n        }\n        const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n        runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n        if (!this.cache) {\n            return this._generateUncached(prompts, callOptions, runnableConfig);\n        }\n        const { cache } = this;\n        const params = this.serialize();\n        params.stop = callOptions.stop ?? params.stop;\n        const llmStringKey = `${Object.entries(params).sort()}`;\n        const missingPromptIndices = [];\n        const generations = await Promise.all(prompts.map(async (prompt, index) => {\n            const result = await cache.lookup(prompt, llmStringKey);\n            if (!result) {\n                missingPromptIndices.push(index);\n            }\n            return result;\n        }));\n        let llmOutput = {};\n        if (missingPromptIndices.length > 0) {\n            const results = await this._generateUncached(missingPromptIndices.map((i) => prompts[i]), callOptions, runnableConfig);\n            await Promise.all(results.generations.map(async (generation, index) => {\n                const promptIndex = missingPromptIndices[index];\n                generations[promptIndex] = generation;\n                return cache.update(prompts[promptIndex], llmStringKey, generation);\n            }));\n            llmOutput = results.llmOutput ?? {};\n        }\n        return { generations, llmOutput };\n    }\n    /**\n     * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n     */\n    async call(prompt, options, callbacks) {\n        const { generations } = await this.generate([prompt], options, callbacks);\n        return generations[0][0].text;\n    }\n    /**\n     * This method is similar to `call`, but it's used for making predictions\n     * based on the input text.\n     * @param text Input text for the prediction.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns A prediction based on the input text.\n     */\n    async predict(text, options, callbacks) {\n        return this.call(text, options, callbacks);\n    }\n    /**\n     * This method takes a list of messages, options, and callbacks, and\n     * returns a predicted message.\n     * @param messages A list of messages for the prediction.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns A predicted message based on the list of messages.\n     */\n    async predictMessages(messages, options, callbacks) {\n        const text = getBufferString(messages);\n        const prediction = await this.call(text, options, callbacks);\n        return new AIMessage(prediction);\n    }\n    /**\n     * Get the identifying parameters of the LLM.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _identifyingParams() {\n        return {};\n    }\n    /**\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this._identifyingParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    _modelType() {\n        return \"base_llm\";\n    }\n    /**\n     * Load an LLM from a json-like object describing it.\n     */\n    static async deserialize(data) {\n        const { _type, _model, ...rest } = data;\n        if (_model && _model !== \"base_llm\") {\n            throw new Error(`Cannot load LLM with model ${_model}`);\n        }\n        const Cls = {\n            openai: (await import(\"./openai.js\")).OpenAI,\n        }[_type];\n        if (Cls === undefined) {\n            throw new Error(`Cannot load  LLM with type ${_type}`);\n        }\n        return new Cls(rest);\n    }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport class LLM extends BaseLLM {\n    async _generate(prompts, options, runManager) {\n        const generations = await Promise.all(prompts.map((prompt, promptIndex) => this._call(prompt, { ...options, promptIndex }, runManager).then((text) => [{ text }])));\n        return { generations };\n    }\n}\n"],"mappings":"AAAA,SAASA,aAAa,QAAQ,mBAAmB;AACjD,SAASC,SAAS,EAAEC,eAAe,EAAEC,OAAO,QAAS,oBAAoB;AACzE,SAASC,iBAAiB,QAAS,2BAA2B;AAC9D,SAASC,eAAe,QAAS,yBAAyB;AAC1D,SAASC,eAAe,QAAQ,mBAAmB;AACnD;AACA;AACA;AACA,OAAO,MAAMC,OAAO,SAASH,iBAAiB,CAAC;EAC3CI,WAAWA,CAAC;IAAEC,KAAK;IAAEC,WAAW;IAAE,GAAGC;EAAK,CAAC,EAAE;IACzC,KAAK,CAACD,WAAW,GAAG;MAAEE,cAAc,EAAEF,WAAW;MAAE,GAAGC;IAAK,CAAC,GAAGA,IAAI,CAAC;IACpEE,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC,WAAW,EAAE,MAAM,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;IAChD,CAAC,CAAC;IACFN,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,OAAO,EAAE;MACjCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,OAAOT,KAAK,KAAK,QAAQ,EAAE;MAC3B,IAAI,CAACA,KAAK,GAAGA,KAAK;IACtB,CAAC,MACI,IAAIA,KAAK,EAAE;MACZ,IAAI,CAACA,KAAK,GAAGT,aAAa,CAACoB,MAAM,CAAC,CAAC;IACvC,CAAC,MACI;MACD,IAAI,CAACX,KAAK,GAAGY,SAAS;IAC1B;EACJ;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMC,MAAMA,CAACC,KAAK,EAAEC,OAAO,EAAE;IACzB,MAAMC,WAAW,GAAGlB,OAAO,CAACmB,0BAA0B,CAACH,KAAK,CAAC;IAC7D,MAAMI,MAAM,GAAG,MAAM,IAAI,CAACC,cAAc,CAAC,CAACH,WAAW,CAAC,EAAED,OAAO,EAAEA,OAAO,EAAEK,SAAS,CAAC;IACpF,OAAOF,MAAM,CAACG,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,IAAI;EACxC;EACA;EACA,OAAOC,qBAAqBA,CAACC,MAAM,EAAEC,QAAQ,EAAEC,WAAW,EAAE;IACxD,MAAM,IAAIC,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACAC,sCAAsCA,CAACb,OAAO,EAAE;IAC5C,MAAM,CAACc,cAAc,EAAEC,WAAW,CAAC,GAAG,KAAK,CAACF,sCAAsC,CAACb,OAAO,CAAC;IAC3F,IAAIe,WAAW,EAAEC,OAAO,IAAI,CAACD,WAAW,CAACE,MAAM,EAAE;MAC7CF,WAAW,CAACE,MAAM,GAAGC,WAAW,CAACF,OAAO,CAACD,WAAW,CAACC,OAAO,CAAC;IACjE;IACA,OAAO,CAACF,cAAc,EAAEC,WAAW,CAAC;EACxC;EACA,OAAOI,eAAeA,CAACpB,KAAK,EAAEC,OAAO,EAAE;IACnC;IACA,IAAI,IAAI,CAACQ,qBAAqB,KAAKzB,OAAO,CAACqC,SAAS,CAACZ,qBAAqB,EAAE;MACxE,MAAM,IAAI,CAACV,MAAM,CAACC,KAAK,EAAEC,OAAO,CAAC;IACrC,CAAC,MACI;MACD,MAAMqB,MAAM,GAAGtC,OAAO,CAACmB,0BAA0B,CAACH,KAAK,CAAC;MACxD,MAAM,CAACe,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACF,sCAAsC,CAACb,OAAO,CAAC;MAC1F,MAAMsB,gBAAgB,GAAG,MAAMzC,eAAe,CAAC0C,SAAS,CAACT,cAAc,CAACT,SAAS,EAAE,IAAI,CAACA,SAAS,EAAES,cAAc,CAACU,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEV,cAAc,CAACW,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;QAAEC,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACrM,MAAMC,KAAK,GAAG;QACV3B,OAAO,EAAEe,WAAW;QACpBa,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAACd,WAAW;MACzD,CAAC;MACD,MAAMe,WAAW,GAAG,MAAMR,gBAAgB,EAAES,cAAc,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAE,CAACX,MAAM,CAACY,QAAQ,CAAC,CAAC,CAAC,EAAEpC,SAAS,EAAEA,SAAS,EAAE8B,KAAK,CAAC;MAC3H,IAAIO,UAAU,GAAG,IAAIxD,eAAe,CAAC;QACjC6B,IAAI,EAAE;MACV,CAAC,CAAC;MACF,IAAI;QACA,WAAW,MAAM4B,KAAK,IAAI,IAAI,CAAC3B,qBAAqB,CAACT,KAAK,CAACkC,QAAQ,CAAC,CAAC,EAAElB,WAAW,EAAEe,WAAW,GAAG,CAAC,CAAC,CAAC,EAAE;UACnG,IAAI,CAACI,UAAU,EAAE;YACbA,UAAU,GAAGC,KAAK;UACtB,CAAC,MACI;YACDD,UAAU,GAAGA,UAAU,CAACE,MAAM,CAACD,KAAK,CAAC;UACzC;UACA,IAAI,OAAOA,KAAK,CAAC5B,IAAI,KAAK,QAAQ,EAAE;YAChC,MAAM4B,KAAK,CAAC5B,IAAI;UACpB;QACJ;MACJ,CAAC,CACD,OAAO8B,GAAG,EAAE;QACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACT,WAAW,IAAI,EAAE,EAAEU,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;QAC3F,MAAMA,GAAG;MACb;MACA,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACT,WAAW,IAAI,EAAE,EAAEU,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEE,YAAY,CAAC;QAC/ErC,WAAW,EAAE,CAAC,CAAC4B,UAAU,CAAC;MAC9B,CAAC,CAAC,CAAC,CAAC;IACR;EACJ;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM9B,cAAcA,CAACwC,YAAY,EAAE5C,OAAO,EAAEK,SAAS,EAAE;IACnD,MAAMwC,OAAO,GAAGD,YAAY,CAACJ,GAAG,CAAEvC,WAAW,IAAKA,WAAW,CAACgC,QAAQ,CAAC,CAAC,CAAC;IACzE,OAAO,IAAI,CAACa,QAAQ,CAACD,OAAO,EAAE7C,OAAO,EAAEK,SAAS,CAAC;EACrD;EACA;AACJ;AACA;EACI;EACAwB,gBAAgBA,CAACnB,QAAQ,EAAE;IACvB,OAAO,CAAC,CAAC;EACb;EACAqC,iBAAiBA,CAACC,SAAS,EAAE;IACzB,MAAMC,UAAU,GAAG,EAAE;IACrB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGF,SAAS,CAAC1C,WAAW,CAAC6C,MAAM,EAAED,CAAC,IAAI,CAAC,EAAE;MACtD,MAAME,OAAO,GAAGJ,SAAS,CAAC1C,WAAW,CAAC4C,CAAC,CAAC;MACxC,IAAIA,CAAC,KAAK,CAAC,EAAE;QACTD,UAAU,CAACI,IAAI,CAAC;UACZ/C,WAAW,EAAE,CAAC8C,OAAO,CAAC;UACtBE,SAAS,EAAEN,SAAS,CAACM;QACzB,CAAC,CAAC;MACN,CAAC,MACI;QACD,MAAMA,SAAS,GAAGN,SAAS,CAACM,SAAS,GAC/B;UAAE,GAAGN,SAAS,CAACM,SAAS;UAAEC,UAAU,EAAE,CAAC;QAAE,CAAC,GAC1C1D,SAAS;QACfoD,UAAU,CAACI,IAAI,CAAC;UACZ/C,WAAW,EAAE,CAAC8C,OAAO,CAAC;UACtBE;QACJ,CAAC,CAAC;MACN;IACJ;IACA,OAAOL,UAAU;EACrB;EACA;EACA,MAAMO,iBAAiBA,CAACX,OAAO,EAAEY,aAAa,EAAEC,cAAc,EAAE;IAC5D,MAAMpC,gBAAgB,GAAG,MAAMzC,eAAe,CAAC0C,SAAS,CAACmC,cAAc,CAACrD,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEqD,cAAc,CAAClC,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEkC,cAAc,CAACjC,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACrM,MAAMC,KAAK,GAAG;MACV3B,OAAO,EAAEyD,aAAa;MACtB7B,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAAC4B,aAAa;IAC3D,CAAC;IACD,MAAM3B,WAAW,GAAG,MAAMR,gBAAgB,EAAES,cAAc,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEa,OAAO,EAAEhD,SAAS,EAAEA,SAAS,EAAE8B,KAAK,CAAC;IAC/G,IAAIgC,MAAM;IACV,IAAI;MACAA,MAAM,GAAG,MAAM,IAAI,CAACC,SAAS,CAACf,OAAO,EAAEY,aAAa,EAAE3B,WAAW,GAAG,CAAC,CAAC,CAAC;IAC3E,CAAC,CACD,OAAOO,GAAG,EAAE;MACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACT,WAAW,IAAI,EAAE,EAAEU,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;MAC3F,MAAMA,GAAG;IACb;IACA,MAAMwB,gBAAgB,GAAG,IAAI,CAACd,iBAAiB,CAACY,MAAM,CAAC;IACvD,MAAMrB,OAAO,CAACC,GAAG,CAAC,CAACT,WAAW,IAAI,EAAE,EAAEU,GAAG,CAAC,CAACC,UAAU,EAAES,CAAC,KAAKT,UAAU,EAAEE,YAAY,CAACkB,gBAAgB,CAACX,CAAC,CAAC,CAAC,CAAC,CAAC;IAC5G,MAAMY,MAAM,GAAGhC,WAAW,EAAEU,GAAG,CAAEuB,OAAO,IAAKA,OAAO,CAACC,KAAK,CAAC,IAAInE,SAAS;IACxE;IACA;IACA;IACAR,MAAM,CAACC,cAAc,CAACqE,MAAM,EAAEhF,OAAO,EAAE;MACnCe,KAAK,EAAEoE,MAAM,GAAG;QAAEA;MAAO,CAAC,GAAGjE,SAAS;MACtCL,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOmE,MAAM;EACjB;EACA;AACJ;AACA;EACI,MAAMb,QAAQA,CAACD,OAAO,EAAE7C,OAAO,EAAEK,SAAS,EAAE;IACxC,IAAI,CAAC4D,KAAK,CAACC,OAAO,CAACrB,OAAO,CAAC,EAAE;MACzB,MAAM,IAAIjC,KAAK,CAAC,iDAAiD,CAAC;IACtE;IACA,IAAI6C,aAAa;IACjB,IAAIQ,KAAK,CAACC,OAAO,CAAClE,OAAO,CAAC,EAAE;MACxByD,aAAa,GAAG;QAAEU,IAAI,EAAEnE;MAAQ,CAAC;IACrC,CAAC,MACI;MACDyD,aAAa,GAAGzD,OAAO;IAC3B;IACA,MAAM,CAACc,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACF,sCAAsC,CAAC4C,aAAa,CAAC;IAChG3C,cAAc,CAACT,SAAS,GAAGS,cAAc,CAACT,SAAS,IAAIA,SAAS;IAChE,IAAI,CAAC,IAAI,CAACpB,KAAK,EAAE;MACb,OAAO,IAAI,CAACuE,iBAAiB,CAACX,OAAO,EAAE9B,WAAW,EAAED,cAAc,CAAC;IACvE;IACA,MAAM;MAAE7B;IAAM,CAAC,GAAG,IAAI;IACtB,MAAMmF,MAAM,GAAG,IAAI,CAACC,SAAS,CAAC,CAAC;IAC/BD,MAAM,CAACD,IAAI,GAAGpD,WAAW,CAACoD,IAAI,IAAIC,MAAM,CAACD,IAAI;IAC7C,MAAMG,YAAY,GAAI,GAAEjF,MAAM,CAACkF,OAAO,CAACH,MAAM,CAAC,CAACI,IAAI,CAAC,CAAE,EAAC;IACvD,MAAMC,oBAAoB,GAAG,EAAE;IAC/B,MAAMnE,WAAW,GAAG,MAAMgC,OAAO,CAACC,GAAG,CAACM,OAAO,CAACL,GAAG,CAAC,OAAOnB,MAAM,EAAEqD,KAAK,KAAK;MACvE,MAAMvE,MAAM,GAAG,MAAMlB,KAAK,CAAC0F,MAAM,CAACtD,MAAM,EAAEiD,YAAY,CAAC;MACvD,IAAI,CAACnE,MAAM,EAAE;QACTsE,oBAAoB,CAACpB,IAAI,CAACqB,KAAK,CAAC;MACpC;MACA,OAAOvE,MAAM;IACjB,CAAC,CAAC,CAAC;IACH,IAAImD,SAAS,GAAG,CAAC,CAAC;IAClB,IAAImB,oBAAoB,CAACtB,MAAM,GAAG,CAAC,EAAE;MACjC,MAAMyB,OAAO,GAAG,MAAM,IAAI,CAACpB,iBAAiB,CAACiB,oBAAoB,CAACjC,GAAG,CAAEU,CAAC,IAAKL,OAAO,CAACK,CAAC,CAAC,CAAC,EAAEnC,WAAW,EAAED,cAAc,CAAC;MACtH,MAAMwB,OAAO,CAACC,GAAG,CAACqC,OAAO,CAACtE,WAAW,CAACkC,GAAG,CAAC,OAAON,UAAU,EAAEwC,KAAK,KAAK;QACnE,MAAMG,WAAW,GAAGJ,oBAAoB,CAACC,KAAK,CAAC;QAC/CpE,WAAW,CAACuE,WAAW,CAAC,GAAG3C,UAAU;QACrC,OAAOjD,KAAK,CAAC6F,MAAM,CAACjC,OAAO,CAACgC,WAAW,CAAC,EAAEP,YAAY,EAAEpC,UAAU,CAAC;MACvE,CAAC,CAAC,CAAC;MACHoB,SAAS,GAAGsB,OAAO,CAACtB,SAAS,IAAI,CAAC,CAAC;IACvC;IACA,OAAO;MAAEhD,WAAW;MAAEgD;IAAU,CAAC;EACrC;EACA;AACJ;AACA;EACI,MAAMyB,IAAIA,CAAC1D,MAAM,EAAErB,OAAO,EAAEK,SAAS,EAAE;IACnC,MAAM;MAAEC;IAAY,CAAC,GAAG,MAAM,IAAI,CAACwC,QAAQ,CAAC,CAACzB,MAAM,CAAC,EAAErB,OAAO,EAAEK,SAAS,CAAC;IACzE,OAAOC,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,IAAI;EACjC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMyE,OAAOA,CAACzE,IAAI,EAAEP,OAAO,EAAEK,SAAS,EAAE;IACpC,OAAO,IAAI,CAAC0E,IAAI,CAACxE,IAAI,EAAEP,OAAO,EAAEK,SAAS,CAAC;EAC9C;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM4E,eAAeA,CAACC,QAAQ,EAAElF,OAAO,EAAEK,SAAS,EAAE;IAChD,MAAME,IAAI,GAAGzB,eAAe,CAACoG,QAAQ,CAAC;IACtC,MAAMC,UAAU,GAAG,MAAM,IAAI,CAACJ,IAAI,CAACxE,IAAI,EAAEP,OAAO,EAAEK,SAAS,CAAC;IAC5D,OAAO,IAAI5B,SAAS,CAAC0G,UAAU,CAAC;EACpC;EACA;AACJ;AACA;EACI;EACAC,kBAAkBA,CAAA,EAAG;IACjB,OAAO,CAAC,CAAC;EACb;EACA;AACJ;AACA;EACIf,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACe,kBAAkB,CAAC,CAAC;MAC5BC,KAAK,EAAE,IAAI,CAAC1F,QAAQ,CAAC,CAAC;MACtB2F,MAAM,EAAE,IAAI,CAACC,UAAU,CAAC;IAC5B,CAAC;EACL;EACAA,UAAUA,CAAA,EAAG;IACT,OAAO,UAAU;EACrB;EACA;AACJ;AACA;EACI,aAAaC,WAAWA,CAACC,IAAI,EAAE;IAC3B,MAAM;MAAEJ,KAAK;MAAEC,MAAM;MAAE,GAAGnG;IAAK,CAAC,GAAGsG,IAAI;IACvC,IAAIH,MAAM,IAAIA,MAAM,KAAK,UAAU,EAAE;MACjC,MAAM,IAAI1E,KAAK,CAAE,8BAA6B0E,MAAO,EAAC,CAAC;IAC3D;IACA,MAAMI,GAAG,GAAG;MACRC,MAAM,EAAE,CAAC,MAAM,MAAM,CAAC,aAAa,CAAC,EAAEC;IAC1C,CAAC,CAACP,KAAK,CAAC;IACR,IAAIK,GAAG,KAAK7F,SAAS,EAAE;MACnB,MAAM,IAAIe,KAAK,CAAE,8BAA6ByE,KAAM,EAAC,CAAC;IAC1D;IACA,OAAO,IAAIK,GAAG,CAACvG,IAAI,CAAC;EACxB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM0G,GAAG,SAAS9G,OAAO,CAAC;EAC7B,MAAM6E,SAASA,CAACf,OAAO,EAAE7C,OAAO,EAAEyC,UAAU,EAAE;IAC1C,MAAMnC,WAAW,GAAG,MAAMgC,OAAO,CAACC,GAAG,CAACM,OAAO,CAACL,GAAG,CAAC,CAACnB,MAAM,EAAEwD,WAAW,KAAK,IAAI,CAACiB,KAAK,CAACzE,MAAM,EAAE;MAAE,GAAGrB,OAAO;MAAE6E;IAAY,CAAC,EAAEpC,UAAU,CAAC,CAACsD,IAAI,CAAExF,IAAI,IAAK,CAAC;MAAEA;IAAK,CAAC,CAAC,CAAC,CAAC,CAAC;IACnK,OAAO;MAAED;IAAY,CAAC;EAC1B;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}