{"ast":null,"code":"import { OpenAI as OpenAIClient } from \"openai\";\nimport { getModelNameForTiktoken } from \"../base_language/count_tokens.js\";\nimport { AIMessage, AIMessageChunk, ChatGenerationChunk, ChatMessage, ChatMessageChunk, FunctionMessageChunk, HumanMessage, HumanMessageChunk, SystemMessage, SystemMessageChunk } from \"../schema/index.js\";\nimport { formatToOpenAIFunction } from \"../tools/convert_to_openai.js\";\nimport { getEndpoint } from \"../util/azure.js\";\nimport { getEnvironmentVariable } from \"../util/env.js\";\nimport { promptLayerTrackRequest } from \"../util/prompt-layer.js\";\nimport { BaseChatModel } from \"./base.js\";\nimport { wrapOpenAIClientError } from \"../util/openai.js\";\nfunction extractGenericMessageCustomRole(message) {\n  if (message.role !== \"system\" && message.role !== \"assistant\" && message.role !== \"user\" && message.role !== \"function\") {\n    console.warn(`Unknown message role: ${message.role}`);\n  }\n  return message.role;\n}\nfunction messageToOpenAIRole(message) {\n  const type = message._getType();\n  switch (type) {\n    case \"system\":\n      return \"system\";\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    case \"function\":\n      return \"function\";\n    case \"generic\":\n      {\n        if (!ChatMessage.isInstance(message)) throw new Error(\"Invalid generic chat message\");\n        return extractGenericMessageCustomRole(message);\n      }\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\nfunction openAIResponseToChatMessage(message) {\n  switch (message.role) {\n    case \"user\":\n      return new HumanMessage(message.content || \"\");\n    case \"assistant\":\n      return new AIMessage(message.content || \"\", {\n        function_call: message.function_call\n      });\n    case \"system\":\n      return new SystemMessage(message.content || \"\");\n    default:\n      return new ChatMessage(message.content || \"\", message.role ?? \"unknown\");\n  }\n}\nfunction _convertDeltaToMessageChunk(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ndelta, defaultRole) {\n  const role = delta.role ?? defaultRole;\n  const content = delta.content ?? \"\";\n  let additional_kwargs;\n  if (delta.function_call) {\n    additional_kwargs = {\n      function_call: delta.function_call\n    };\n  } else {\n    additional_kwargs = {};\n  }\n  if (role === \"user\") {\n    return new HumanMessageChunk({\n      content\n    });\n  } else if (role === \"assistant\") {\n    return new AIMessageChunk({\n      content,\n      additional_kwargs\n    });\n  } else if (role === \"system\") {\n    return new SystemMessageChunk({\n      content\n    });\n  } else if (role === \"function\") {\n    return new FunctionMessageChunk({\n      content,\n      additional_kwargs,\n      name: delta.name\n    });\n  } else {\n    return new ChatMessageChunk({\n      content,\n      role\n    });\n  }\n}\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n * `AZURE_OPENAI_BASE_PATH` is optional and will override `AZURE_OPENAI_API_INSTANCE_NAME` if you need to use a custom endpoint.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createChatCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n */\nexport class ChatOpenAI extends BaseChatModel {\n  static lc_name() {\n    return \"ChatOpenAI\";\n  }\n  get callKeys() {\n    return [...super.callKeys, \"options\", \"function_call\", \"functions\", \"tools\", \"promptIndex\"];\n  }\n  get lc_secrets() {\n    return {\n      openAIApiKey: \"OPENAI_API_KEY\",\n      azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\"\n    };\n  }\n  get lc_aliases() {\n    return {\n      modelName: \"model\",\n      openAIApiKey: \"openai_api_key\",\n      azureOpenAIApiVersion: \"azure_openai_api_version\",\n      azureOpenAIApiKey: \"azure_openai_api_key\",\n      azureOpenAIApiInstanceName: \"azure_openai_api_instance_name\",\n      azureOpenAIApiDeploymentName: \"azure_openai_api_deployment_name\"\n    };\n  }\n  constructor(fields, /** @deprecated */\n  configuration) {\n    super(fields ?? {});\n    Object.defineProperty(this, \"lc_serializable\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: true\n    });\n    Object.defineProperty(this, \"temperature\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"topP\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"frequencyPenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"presencePenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"n\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"logitBias\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"modelName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"gpt-3.5-turbo\"\n    });\n    Object.defineProperty(this, \"modelKwargs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"stop\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"user\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"timeout\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"streaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"maxTokens\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"openAIApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIBasePath\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"organization\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"client\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"clientConfig\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.openAIApiKey = fields?.openAIApiKey ?? getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.azureOpenAIApiKey = fields?.azureOpenAIApiKey ?? getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n    if (!this.azureOpenAIApiKey && !this.openAIApiKey) {\n      throw new Error(\"OpenAI or Azure OpenAI API key not found\");\n    }\n    this.azureOpenAIApiInstanceName = fields?.azureOpenAIApiInstanceName ?? getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n    this.azureOpenAIApiDeploymentName = fields?.azureOpenAIApiDeploymentName ?? getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\");\n    this.azureOpenAIApiVersion = fields?.azureOpenAIApiVersion ?? getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n    this.azureOpenAIBasePath = fields?.azureOpenAIBasePath ?? getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n    this.organization = fields?.configuration?.organization ?? getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n    this.modelName = fields?.modelName ?? this.modelName;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.timeout = fields?.timeout;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.maxTokens = fields?.maxTokens;\n    this.n = fields?.n ?? this.n;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stop;\n    this.user = fields?.user;\n    this.streaming = fields?.streaming ?? false;\n    if (this.azureOpenAIApiKey) {\n      if (!this.azureOpenAIApiInstanceName && !this.azureOpenAIBasePath) {\n        throw new Error(\"Azure OpenAI API instance name not found\");\n      }\n      if (!this.azureOpenAIApiDeploymentName) {\n        throw new Error(\"Azure OpenAI API deployment name not found\");\n      }\n      if (!this.azureOpenAIApiVersion) {\n        throw new Error(\"Azure OpenAI API version not found\");\n      }\n      this.openAIApiKey = this.openAIApiKey ?? \"\";\n    }\n    this.clientConfig = {\n      apiKey: this.openAIApiKey,\n      organization: this.organization,\n      baseURL: configuration?.basePath ?? fields?.configuration?.basePath,\n      dangerouslyAllowBrowser: true,\n      defaultHeaders: configuration?.baseOptions?.headers ?? fields?.configuration?.baseOptions?.headers,\n      defaultQuery: configuration?.baseOptions?.params ?? fields?.configuration?.baseOptions?.params,\n      ...configuration,\n      ...fields?.configuration\n    };\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(options) {\n    return {\n      model: this.modelName,\n      temperature: this.temperature,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n      n: this.n,\n      logit_bias: this.logitBias,\n      stop: options?.stop ?? this.stop,\n      user: this.user,\n      stream: this.streaming,\n      functions: options?.functions ?? (options?.tools ? options?.tools.map(formatToOpenAIFunction) : undefined),\n      function_call: options?.function_call,\n      ...this.modelKwargs\n    };\n  }\n  /** @ignore */\n  _identifyingParams() {\n    return {\n      model_name: this.modelName,\n      ...this.invocationParams(),\n      ...this.clientConfig\n    };\n  }\n  async *_streamResponseChunks(messages, options, runManager) {\n    const messagesMapped = messages.map(message => ({\n      role: messageToOpenAIRole(message),\n      content: message.content,\n      name: message.name,\n      function_call: message.additional_kwargs.function_call\n    }));\n    const params = {\n      ...this.invocationParams(options),\n      messages: messagesMapped,\n      stream: true\n    };\n    let defaultRole;\n    const streamIterable = await this.completionWithRetry(params, options);\n    for await (const data of streamIterable) {\n      const choice = data.choices[0];\n      if (!choice) {\n        continue;\n      }\n      const {\n        delta\n      } = choice;\n      const chunk = _convertDeltaToMessageChunk(delta, defaultRole);\n      defaultRole = delta.role ?? defaultRole;\n      const newTokenIndices = {\n        prompt: options.promptIndex ?? 0,\n        completion: choice.index ?? 0\n      };\n      const generationChunk = new ChatGenerationChunk({\n        message: chunk,\n        text: chunk.content,\n        generationInfo: newTokenIndices\n      });\n      yield generationChunk;\n      // eslint-disable-next-line no-void\n      void runManager?.handleLLMNewToken(generationChunk.text ?? \"\", newTokenIndices, undefined, undefined, undefined, {\n        chunk: generationChunk\n      });\n    }\n    if (options.signal?.aborted) {\n      throw new Error(\"AbortError\");\n    }\n  }\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n  /** @ignore */\n  async _generate(messages, options, runManager) {\n    const tokenUsage = {};\n    const params = this.invocationParams(options);\n    const messagesMapped = messages.map(message => ({\n      role: messageToOpenAIRole(message),\n      content: message.content,\n      name: message.name,\n      function_call: message.additional_kwargs.function_call\n    }));\n    if (params.stream) {\n      const stream = await this._streamResponseChunks(messages, options, runManager);\n      const finalChunks = {};\n      for await (const chunk of stream) {\n        const index = chunk.generationInfo?.completion ?? 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks).sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10)).map(([_, value]) => value);\n      return {\n        generations\n      };\n    } else {\n      const data = await this.completionWithRetry({\n        ...params,\n        stream: false,\n        messages: messagesMapped\n      }, {\n        signal: options?.signal,\n        ...options?.options\n      });\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens\n      } = data?.usage ?? {};\n      if (completionTokens) {\n        tokenUsage.completionTokens = (tokenUsage.completionTokens ?? 0) + completionTokens;\n      }\n      if (promptTokens) {\n        tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n      }\n      if (totalTokens) {\n        tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n      }\n      const generations = [];\n      for (const part of data?.choices ?? []) {\n        const text = part.message?.content ?? \"\";\n        const generation = {\n          text,\n          message: openAIResponseToChatMessage(part.message ?? {\n            role: \"assistant\"\n          })\n        };\n        if (part.finish_reason) {\n          generation.generationInfo = {\n            finish_reason: part.finish_reason\n          };\n        }\n        generations.push(generation);\n      }\n      return {\n        generations,\n        llmOutput: {\n          tokenUsage\n        }\n      };\n    }\n  }\n  async getNumTokensFromMessages(messages) {\n    let totalCount = 0;\n    let tokensPerMessage = 0;\n    let tokensPerName = 0;\n    // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n    if (getModelNameForTiktoken(this.modelName) === \"gpt-3.5-turbo\") {\n      tokensPerMessage = 4;\n      tokensPerName = -1;\n    } else if (getModelNameForTiktoken(this.modelName).startsWith(\"gpt-4\")) {\n      tokensPerMessage = 3;\n      tokensPerName = 1;\n    }\n    const countPerMessage = await Promise.all(messages.map(async message => {\n      const textCount = await this.getNumTokens(message.content);\n      const roleCount = await this.getNumTokens(messageToOpenAIRole(message));\n      const nameCount = message.name !== undefined ? tokensPerName + (await this.getNumTokens(message.name)) : 0;\n      const count = textCount + tokensPerMessage + roleCount + nameCount;\n      totalCount += count;\n      return count;\n    }));\n    totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n    return {\n      totalCount,\n      countPerMessage\n    };\n  }\n  async completionWithRetry(request, options) {\n    const requestOptions = this._getClientOptions(options);\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.chat.completions.create(request, requestOptions);\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n  _getClientOptions(options) {\n    if (!this.client) {\n      const openAIEndpointConfig = {\n        azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n        azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n        azureOpenAIApiKey: this.azureOpenAIApiKey,\n        azureOpenAIBasePath: this.azureOpenAIBasePath,\n        baseURL: this.clientConfig.baseURL\n      };\n      const endpoint = getEndpoint(openAIEndpointConfig);\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0\n      };\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options\n    };\n    if (this.azureOpenAIApiKey) {\n      requestOptions.headers = {\n        \"api-key\": this.azureOpenAIApiKey,\n        ...requestOptions.headers\n      };\n      requestOptions.query = {\n        \"api-version\": this.azureOpenAIApiVersion,\n        ...requestOptions.query\n      };\n    }\n    return requestOptions;\n  }\n  _llmType() {\n    return \"openai\";\n  }\n  /** @ignore */\n  _combineLLMOutput(...llmOutputs) {\n    return llmOutputs.reduce((acc, llmOutput) => {\n      if (llmOutput && llmOutput.tokenUsage) {\n        acc.tokenUsage.completionTokens += llmOutput.tokenUsage.completionTokens ?? 0;\n        acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n        acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n      }\n      return acc;\n    }, {\n      tokenUsage: {\n        completionTokens: 0,\n        promptTokens: 0,\n        totalTokens: 0\n      }\n    });\n  }\n}\nexport class PromptLayerChatOpenAI extends ChatOpenAI {\n  constructor(fields) {\n    super(fields);\n    Object.defineProperty(this, \"promptLayerApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"plTags\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"returnPromptLayerId\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.promptLayerApiKey = fields?.promptLayerApiKey ?? (typeof process !== \"undefined\" ?\n    // eslint-disable-next-line no-process-env\n    process.env?.PROMPTLAYER_API_KEY : undefined);\n    this.plTags = fields?.plTags ?? [];\n    this.returnPromptLayerId = fields?.returnPromptLayerId ?? false;\n  }\n  async _generate(messages, options, runManager) {\n    const requestStartTime = Date.now();\n    let parsedOptions;\n    if (Array.isArray(options)) {\n      parsedOptions = {\n        stop: options\n      };\n    } else if (options?.timeout && !options.signal) {\n      parsedOptions = {\n        ...options,\n        signal: AbortSignal.timeout(options.timeout)\n      };\n    } else {\n      parsedOptions = options ?? {};\n    }\n    const generatedResponses = await super._generate(messages, parsedOptions, runManager);\n    const requestEndTime = Date.now();\n    const _convertMessageToDict = message => {\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      let messageDict;\n      if (message._getType() === \"human\") {\n        messageDict = {\n          role: \"user\",\n          content: message.content\n        };\n      } else if (message._getType() === \"ai\") {\n        messageDict = {\n          role: \"assistant\",\n          content: message.content\n        };\n      } else if (message._getType() === \"function\") {\n        messageDict = {\n          role: \"assistant\",\n          content: message.content\n        };\n      } else if (message._getType() === \"system\") {\n        messageDict = {\n          role: \"system\",\n          content: message.content\n        };\n      } else if (message._getType() === \"generic\") {\n        messageDict = {\n          role: message.role,\n          content: message.content\n        };\n      } else {\n        throw new Error(`Got unknown type ${message}`);\n      }\n      return messageDict;\n    };\n    const _createMessageDicts = (messages, callOptions) => {\n      const params = {\n        ...this.invocationParams(),\n        model: this.modelName\n      };\n      if (callOptions?.stop) {\n        if (Object.keys(params).includes(\"stop\")) {\n          throw new Error(\"`stop` found in both the input and default params.\");\n        }\n      }\n      const messageDicts = messages.map(message => _convertMessageToDict(message));\n      return messageDicts;\n    };\n    for (let i = 0; i < generatedResponses.generations.length; i += 1) {\n      const generation = generatedResponses.generations[i];\n      const messageDicts = _createMessageDicts(messages, parsedOptions);\n      let promptLayerRequestId;\n      const parsedResp = [{\n        content: generation.text,\n        role: messageToOpenAIRole(generation.message)\n      }];\n      const promptLayerRespBody = await promptLayerTrackRequest(this.caller, \"langchain.PromptLayerChatOpenAI\", {\n        ...this._identifyingParams(),\n        messages: messageDicts,\n        stream: false\n      }, this.plTags, parsedResp, requestStartTime, requestEndTime, this.promptLayerApiKey);\n      if (this.returnPromptLayerId === true) {\n        if (promptLayerRespBody.success === true) {\n          promptLayerRequestId = promptLayerRespBody.request_id;\n        }\n        if (!generation.generationInfo || typeof generation.generationInfo !== \"object\") {\n          generation.generationInfo = {};\n        }\n        generation.generationInfo.promptLayerRequestId = promptLayerRequestId;\n      }\n    }\n    return generatedResponses;\n  }\n}","map":{"version":3,"names":["OpenAI","OpenAIClient","getModelNameForTiktoken","AIMessage","AIMessageChunk","ChatGenerationChunk","ChatMessage","ChatMessageChunk","FunctionMessageChunk","HumanMessage","HumanMessageChunk","SystemMessage","SystemMessageChunk","formatToOpenAIFunction","getEndpoint","getEnvironmentVariable","promptLayerTrackRequest","BaseChatModel","wrapOpenAIClientError","extractGenericMessageCustomRole","message","role","console","warn","messageToOpenAIRole","type","_getType","isInstance","Error","openAIResponseToChatMessage","content","function_call","_convertDeltaToMessageChunk","delta","defaultRole","additional_kwargs","name","ChatOpenAI","lc_name","callKeys","lc_secrets","openAIApiKey","azureOpenAIApiKey","organization","lc_aliases","modelName","azureOpenAIApiVersion","azureOpenAIApiInstanceName","azureOpenAIApiDeploymentName","constructor","fields","configuration","Object","defineProperty","enumerable","configurable","writable","value","azureOpenAIBasePath","modelKwargs","timeout","temperature","topP","frequencyPenalty","presencePenalty","maxTokens","n","logitBias","stop","user","streaming","clientConfig","apiKey","baseURL","basePath","dangerouslyAllowBrowser","defaultHeaders","baseOptions","headers","defaultQuery","params","invocationParams","options","model","top_p","frequency_penalty","presence_penalty","max_tokens","undefined","logit_bias","stream","functions","tools","map","_identifyingParams","model_name","_streamResponseChunks","messages","runManager","messagesMapped","streamIterable","completionWithRetry","data","choice","choices","chunk","newTokenIndices","prompt","promptIndex","completion","index","generationChunk","text","generationInfo","handleLLMNewToken","signal","aborted","identifyingParams","_generate","tokenUsage","finalChunks","concat","generations","entries","sort","aKey","bKey","parseInt","_","completion_tokens","completionTokens","prompt_tokens","promptTokens","total_tokens","totalTokens","usage","part","generation","finish_reason","push","llmOutput","getNumTokensFromMessages","totalCount","tokensPerMessage","tokensPerName","startsWith","countPerMessage","Promise","all","textCount","getNumTokens","roleCount","nameCount","count","request","requestOptions","_getClientOptions","caller","call","res","client","chat","completions","create","e","error","openAIEndpointConfig","endpoint","maxRetries","query","_llmType","_combineLLMOutput","llmOutputs","reduce","acc","PromptLayerChatOpenAI","promptLayerApiKey","process","env","PROMPTLAYER_API_KEY","plTags","returnPromptLayerId","requestStartTime","Date","now","parsedOptions","Array","isArray","AbortSignal","generatedResponses","requestEndTime","_convertMessageToDict","messageDict","_createMessageDicts","callOptions","keys","includes","messageDicts","i","length","promptLayerRequestId","parsedResp","promptLayerRespBody","success","request_id"],"sources":["/Users/mayamagavi/instalily/case-study/node_modules/langchain/dist/chat_models/openai.js"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport { getModelNameForTiktoken } from \"../base_language/count_tokens.js\";\nimport { AIMessage, AIMessageChunk, ChatGenerationChunk, ChatMessage, ChatMessageChunk, FunctionMessageChunk, HumanMessage, HumanMessageChunk, SystemMessage, SystemMessageChunk, } from \"../schema/index.js\";\nimport { formatToOpenAIFunction } from \"../tools/convert_to_openai.js\";\nimport { getEndpoint } from \"../util/azure.js\";\nimport { getEnvironmentVariable } from \"../util/env.js\";\nimport { promptLayerTrackRequest } from \"../util/prompt-layer.js\";\nimport { BaseChatModel } from \"./base.js\";\nimport { wrapOpenAIClientError } from \"../util/openai.js\";\nfunction extractGenericMessageCustomRole(message) {\n    if (message.role !== \"system\" &&\n        message.role !== \"assistant\" &&\n        message.role !== \"user\" &&\n        message.role !== \"function\") {\n        console.warn(`Unknown message role: ${message.role}`);\n    }\n    return message.role;\n}\nfunction messageToOpenAIRole(message) {\n    const type = message._getType();\n    switch (type) {\n        case \"system\":\n            return \"system\";\n        case \"ai\":\n            return \"assistant\";\n        case \"human\":\n            return \"user\";\n        case \"function\":\n            return \"function\";\n        case \"generic\": {\n            if (!ChatMessage.isInstance(message))\n                throw new Error(\"Invalid generic chat message\");\n            return extractGenericMessageCustomRole(message);\n        }\n        default:\n            throw new Error(`Unknown message type: ${type}`);\n    }\n}\nfunction openAIResponseToChatMessage(message) {\n    switch (message.role) {\n        case \"user\":\n            return new HumanMessage(message.content || \"\");\n        case \"assistant\":\n            return new AIMessage(message.content || \"\", {\n                function_call: message.function_call,\n            });\n        case \"system\":\n            return new SystemMessage(message.content || \"\");\n        default:\n            return new ChatMessage(message.content || \"\", message.role ?? \"unknown\");\n    }\n}\nfunction _convertDeltaToMessageChunk(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ndelta, defaultRole) {\n    const role = delta.role ?? defaultRole;\n    const content = delta.content ?? \"\";\n    let additional_kwargs;\n    if (delta.function_call) {\n        additional_kwargs = {\n            function_call: delta.function_call,\n        };\n    }\n    else {\n        additional_kwargs = {};\n    }\n    if (role === \"user\") {\n        return new HumanMessageChunk({ content });\n    }\n    else if (role === \"assistant\") {\n        return new AIMessageChunk({ content, additional_kwargs });\n    }\n    else if (role === \"system\") {\n        return new SystemMessageChunk({ content });\n    }\n    else if (role === \"function\") {\n        return new FunctionMessageChunk({\n            content,\n            additional_kwargs,\n            name: delta.name,\n        });\n    }\n    else {\n        return new ChatMessageChunk({ content, role });\n    }\n}\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n * `AZURE_OPENAI_BASE_PATH` is optional and will override `AZURE_OPENAI_API_INSTANCE_NAME` if you need to use a custom endpoint.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createChatCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n */\nexport class ChatOpenAI extends BaseChatModel {\n    static lc_name() {\n        return \"ChatOpenAI\";\n    }\n    get callKeys() {\n        return [\n            ...super.callKeys,\n            \"options\",\n            \"function_call\",\n            \"functions\",\n            \"tools\",\n            \"promptIndex\",\n        ];\n    }\n    get lc_secrets() {\n        return {\n            openAIApiKey: \"OPENAI_API_KEY\",\n            azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n            organization: \"OPENAI_ORGANIZATION\",\n        };\n    }\n    get lc_aliases() {\n        return {\n            modelName: \"model\",\n            openAIApiKey: \"openai_api_key\",\n            azureOpenAIApiVersion: \"azure_openai_api_version\",\n            azureOpenAIApiKey: \"azure_openai_api_key\",\n            azureOpenAIApiInstanceName: \"azure_openai_api_instance_name\",\n            azureOpenAIApiDeploymentName: \"azure_openai_api_deployment_name\",\n        };\n    }\n    constructor(fields, \n    /** @deprecated */\n    configuration) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"gpt-3.5-turbo\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"user\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"openAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIBasePath\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"organization\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.openAIApiKey =\n            fields?.openAIApiKey ?? getEnvironmentVariable(\"OPENAI_API_KEY\");\n        this.azureOpenAIApiKey =\n            fields?.azureOpenAIApiKey ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n        if (!this.azureOpenAIApiKey && !this.openAIApiKey) {\n            throw new Error(\"OpenAI or Azure OpenAI API key not found\");\n        }\n        this.azureOpenAIApiInstanceName =\n            fields?.azureOpenAIApiInstanceName ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n        this.azureOpenAIApiDeploymentName =\n            fields?.azureOpenAIApiDeploymentName ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\");\n        this.azureOpenAIApiVersion =\n            fields?.azureOpenAIApiVersion ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n        this.azureOpenAIBasePath =\n            fields?.azureOpenAIBasePath ??\n                getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n        this.organization =\n            fields?.configuration?.organization ??\n                getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.maxTokens = fields?.maxTokens;\n        this.n = fields?.n ?? this.n;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stop;\n        this.user = fields?.user;\n        this.streaming = fields?.streaming ?? false;\n        if (this.azureOpenAIApiKey) {\n            if (!this.azureOpenAIApiInstanceName && !this.azureOpenAIBasePath) {\n                throw new Error(\"Azure OpenAI API instance name not found\");\n            }\n            if (!this.azureOpenAIApiDeploymentName) {\n                throw new Error(\"Azure OpenAI API deployment name not found\");\n            }\n            if (!this.azureOpenAIApiVersion) {\n                throw new Error(\"Azure OpenAI API version not found\");\n            }\n            this.openAIApiKey = this.openAIApiKey ?? \"\";\n        }\n        this.clientConfig = {\n            apiKey: this.openAIApiKey,\n            organization: this.organization,\n            baseURL: configuration?.basePath ?? fields?.configuration?.basePath,\n            dangerouslyAllowBrowser: true,\n            defaultHeaders: configuration?.baseOptions?.headers ??\n                fields?.configuration?.baseOptions?.headers,\n            defaultQuery: configuration?.baseOptions?.params ??\n                fields?.configuration?.baseOptions?.params,\n            ...configuration,\n            ...fields?.configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(options) {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n            n: this.n,\n            logit_bias: this.logitBias,\n            stop: options?.stop ?? this.stop,\n            user: this.user,\n            stream: this.streaming,\n            functions: options?.functions ??\n                (options?.tools\n                    ? options?.tools.map(formatToOpenAIFunction)\n                    : undefined),\n            function_call: options?.function_call,\n            ...this.modelKwargs,\n        };\n    }\n    /** @ignore */\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    async *_streamResponseChunks(messages, options, runManager) {\n        const messagesMapped = messages.map((message) => ({\n            role: messageToOpenAIRole(message),\n            content: message.content,\n            name: message.name,\n            function_call: message.additional_kwargs\n                .function_call,\n        }));\n        const params = {\n            ...this.invocationParams(options),\n            messages: messagesMapped,\n            stream: true,\n        };\n        let defaultRole;\n        const streamIterable = await this.completionWithRetry(params, options);\n        for await (const data of streamIterable) {\n            const choice = data.choices[0];\n            if (!choice) {\n                continue;\n            }\n            const { delta } = choice;\n            const chunk = _convertDeltaToMessageChunk(delta, defaultRole);\n            defaultRole = delta.role ?? defaultRole;\n            const newTokenIndices = {\n                prompt: options.promptIndex ?? 0,\n                completion: choice.index ?? 0,\n            };\n            const generationChunk = new ChatGenerationChunk({\n                message: chunk,\n                text: chunk.content,\n                generationInfo: newTokenIndices,\n            });\n            yield generationChunk;\n            // eslint-disable-next-line no-void\n            void runManager?.handleLLMNewToken(generationChunk.text ?? \"\", newTokenIndices, undefined, undefined, undefined, { chunk: generationChunk });\n        }\n        if (options.signal?.aborted) {\n            throw new Error(\"AbortError\");\n        }\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /** @ignore */\n    async _generate(messages, options, runManager) {\n        const tokenUsage = {};\n        const params = this.invocationParams(options);\n        const messagesMapped = messages.map((message) => ({\n            role: messageToOpenAIRole(message),\n            content: message.content,\n            name: message.name,\n            function_call: message.additional_kwargs\n                .function_call,\n        }));\n        if (params.stream) {\n            const stream = await this._streamResponseChunks(messages, options, runManager);\n            const finalChunks = {};\n            for await (const chunk of stream) {\n                const index = chunk.generationInfo?.completion ?? 0;\n                if (finalChunks[index] === undefined) {\n                    finalChunks[index] = chunk;\n                }\n                else {\n                    finalChunks[index] = finalChunks[index].concat(chunk);\n                }\n            }\n            const generations = Object.entries(finalChunks)\n                .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n                .map(([_, value]) => value);\n            return { generations };\n        }\n        else {\n            const data = await this.completionWithRetry({\n                ...params,\n                stream: false,\n                messages: messagesMapped,\n            }, {\n                signal: options?.signal,\n                ...options?.options,\n            });\n            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data?.usage ?? {};\n            if (completionTokens) {\n                tokenUsage.completionTokens =\n                    (tokenUsage.completionTokens ?? 0) + completionTokens;\n            }\n            if (promptTokens) {\n                tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n            }\n            if (totalTokens) {\n                tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n            }\n            const generations = [];\n            for (const part of data?.choices ?? []) {\n                const text = part.message?.content ?? \"\";\n                const generation = {\n                    text,\n                    message: openAIResponseToChatMessage(part.message ?? { role: \"assistant\" }),\n                };\n                if (part.finish_reason) {\n                    generation.generationInfo = { finish_reason: part.finish_reason };\n                }\n                generations.push(generation);\n            }\n            return {\n                generations,\n                llmOutput: { tokenUsage },\n            };\n        }\n    }\n    async getNumTokensFromMessages(messages) {\n        let totalCount = 0;\n        let tokensPerMessage = 0;\n        let tokensPerName = 0;\n        // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n        if (getModelNameForTiktoken(this.modelName) === \"gpt-3.5-turbo\") {\n            tokensPerMessage = 4;\n            tokensPerName = -1;\n        }\n        else if (getModelNameForTiktoken(this.modelName).startsWith(\"gpt-4\")) {\n            tokensPerMessage = 3;\n            tokensPerName = 1;\n        }\n        const countPerMessage = await Promise.all(messages.map(async (message) => {\n            const textCount = await this.getNumTokens(message.content);\n            const roleCount = await this.getNumTokens(messageToOpenAIRole(message));\n            const nameCount = message.name !== undefined\n                ? tokensPerName + (await this.getNumTokens(message.name))\n                : 0;\n            const count = textCount + tokensPerMessage + roleCount + nameCount;\n            totalCount += count;\n            return count;\n        }));\n        totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n        return { totalCount, countPerMessage };\n    }\n    async completionWithRetry(request, options) {\n        const requestOptions = this._getClientOptions(options);\n        return this.caller.call(async () => {\n            try {\n                const res = await this.client.chat.completions.create(request, requestOptions);\n                return res;\n            }\n            catch (e) {\n                const error = wrapOpenAIClientError(e);\n                throw error;\n            }\n        });\n    }\n    _getClientOptions(options) {\n        if (!this.client) {\n            const openAIEndpointConfig = {\n                azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n                azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n                azureOpenAIApiKey: this.azureOpenAIApiKey,\n                azureOpenAIBasePath: this.azureOpenAIBasePath,\n                baseURL: this.clientConfig.baseURL,\n            };\n            const endpoint = getEndpoint(openAIEndpointConfig);\n            const params = {\n                ...this.clientConfig,\n                baseURL: endpoint,\n                timeout: this.timeout,\n                maxRetries: 0,\n            };\n            if (!params.baseURL) {\n                delete params.baseURL;\n            }\n            this.client = new OpenAIClient(params);\n        }\n        const requestOptions = {\n            ...this.clientConfig,\n            ...options,\n        };\n        if (this.azureOpenAIApiKey) {\n            requestOptions.headers = {\n                \"api-key\": this.azureOpenAIApiKey,\n                ...requestOptions.headers,\n            };\n            requestOptions.query = {\n                \"api-version\": this.azureOpenAIApiVersion,\n                ...requestOptions.query,\n            };\n        }\n        return requestOptions;\n    }\n    _llmType() {\n        return \"openai\";\n    }\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs) {\n        return llmOutputs.reduce((acc, llmOutput) => {\n            if (llmOutput && llmOutput.tokenUsage) {\n                acc.tokenUsage.completionTokens +=\n                    llmOutput.tokenUsage.completionTokens ?? 0;\n                acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n                acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n            }\n            return acc;\n        }, {\n            tokenUsage: {\n                completionTokens: 0,\n                promptTokens: 0,\n                totalTokens: 0,\n            },\n        });\n    }\n}\nexport class PromptLayerChatOpenAI extends ChatOpenAI {\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"promptLayerApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"plTags\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"returnPromptLayerId\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.promptLayerApiKey =\n            fields?.promptLayerApiKey ??\n                (typeof process !== \"undefined\"\n                    ? // eslint-disable-next-line no-process-env\n                        process.env?.PROMPTLAYER_API_KEY\n                    : undefined);\n        this.plTags = fields?.plTags ?? [];\n        this.returnPromptLayerId = fields?.returnPromptLayerId ?? false;\n    }\n    async _generate(messages, options, runManager) {\n        const requestStartTime = Date.now();\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else if (options?.timeout && !options.signal) {\n            parsedOptions = {\n                ...options,\n                signal: AbortSignal.timeout(options.timeout),\n            };\n        }\n        else {\n            parsedOptions = options ?? {};\n        }\n        const generatedResponses = await super._generate(messages, parsedOptions, runManager);\n        const requestEndTime = Date.now();\n        const _convertMessageToDict = (message) => {\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            let messageDict;\n            if (message._getType() === \"human\") {\n                messageDict = { role: \"user\", content: message.content };\n            }\n            else if (message._getType() === \"ai\") {\n                messageDict = { role: \"assistant\", content: message.content };\n            }\n            else if (message._getType() === \"function\") {\n                messageDict = { role: \"assistant\", content: message.content };\n            }\n            else if (message._getType() === \"system\") {\n                messageDict = { role: \"system\", content: message.content };\n            }\n            else if (message._getType() === \"generic\") {\n                messageDict = {\n                    role: message.role,\n                    content: message.content,\n                };\n            }\n            else {\n                throw new Error(`Got unknown type ${message}`);\n            }\n            return messageDict;\n        };\n        const _createMessageDicts = (messages, callOptions) => {\n            const params = {\n                ...this.invocationParams(),\n                model: this.modelName,\n            };\n            if (callOptions?.stop) {\n                if (Object.keys(params).includes(\"stop\")) {\n                    throw new Error(\"`stop` found in both the input and default params.\");\n                }\n            }\n            const messageDicts = messages.map((message) => _convertMessageToDict(message));\n            return messageDicts;\n        };\n        for (let i = 0; i < generatedResponses.generations.length; i += 1) {\n            const generation = generatedResponses.generations[i];\n            const messageDicts = _createMessageDicts(messages, parsedOptions);\n            let promptLayerRequestId;\n            const parsedResp = [\n                {\n                    content: generation.text,\n                    role: messageToOpenAIRole(generation.message),\n                },\n            ];\n            const promptLayerRespBody = await promptLayerTrackRequest(this.caller, \"langchain.PromptLayerChatOpenAI\", { ...this._identifyingParams(), messages: messageDicts, stream: false }, this.plTags, parsedResp, requestStartTime, requestEndTime, this.promptLayerApiKey);\n            if (this.returnPromptLayerId === true) {\n                if (promptLayerRespBody.success === true) {\n                    promptLayerRequestId = promptLayerRespBody.request_id;\n                }\n                if (!generation.generationInfo ||\n                    typeof generation.generationInfo !== \"object\") {\n                    generation.generationInfo = {};\n                }\n                generation.generationInfo.promptLayerRequestId = promptLayerRequestId;\n            }\n        }\n        return generatedResponses;\n    }\n}\n"],"mappings":"AAAA,SAASA,MAAM,IAAIC,YAAY,QAAQ,QAAQ;AAC/C,SAASC,uBAAuB,QAAQ,kCAAkC;AAC1E,SAASC,SAAS,EAAEC,cAAc,EAAEC,mBAAmB,EAAEC,WAAW,EAAEC,gBAAgB,EAAEC,oBAAoB,EAAEC,YAAY,EAAEC,iBAAiB,EAAEC,aAAa,EAAEC,kBAAkB,QAAS,oBAAoB;AAC7M,SAASC,sBAAsB,QAAQ,+BAA+B;AACtE,SAASC,WAAW,QAAQ,kBAAkB;AAC9C,SAASC,sBAAsB,QAAQ,gBAAgB;AACvD,SAASC,uBAAuB,QAAQ,yBAAyB;AACjE,SAASC,aAAa,QAAQ,WAAW;AACzC,SAASC,qBAAqB,QAAQ,mBAAmB;AACzD,SAASC,+BAA+BA,CAACC,OAAO,EAAE;EAC9C,IAAIA,OAAO,CAACC,IAAI,KAAK,QAAQ,IACzBD,OAAO,CAACC,IAAI,KAAK,WAAW,IAC5BD,OAAO,CAACC,IAAI,KAAK,MAAM,IACvBD,OAAO,CAACC,IAAI,KAAK,UAAU,EAAE;IAC7BC,OAAO,CAACC,IAAI,CAAE,yBAAwBH,OAAO,CAACC,IAAK,EAAC,CAAC;EACzD;EACA,OAAOD,OAAO,CAACC,IAAI;AACvB;AACA,SAASG,mBAAmBA,CAACJ,OAAO,EAAE;EAClC,MAAMK,IAAI,GAAGL,OAAO,CAACM,QAAQ,CAAC,CAAC;EAC/B,QAAQD,IAAI;IACR,KAAK,QAAQ;MACT,OAAO,QAAQ;IACnB,KAAK,IAAI;MACL,OAAO,WAAW;IACtB,KAAK,OAAO;MACR,OAAO,MAAM;IACjB,KAAK,UAAU;MACX,OAAO,UAAU;IACrB,KAAK,SAAS;MAAE;QACZ,IAAI,CAACnB,WAAW,CAACqB,UAAU,CAACP,OAAO,CAAC,EAChC,MAAM,IAAIQ,KAAK,CAAC,8BAA8B,CAAC;QACnD,OAAOT,+BAA+B,CAACC,OAAO,CAAC;MACnD;IACA;MACI,MAAM,IAAIQ,KAAK,CAAE,yBAAwBH,IAAK,EAAC,CAAC;EACxD;AACJ;AACA,SAASI,2BAA2BA,CAACT,OAAO,EAAE;EAC1C,QAAQA,OAAO,CAACC,IAAI;IAChB,KAAK,MAAM;MACP,OAAO,IAAIZ,YAAY,CAACW,OAAO,CAACU,OAAO,IAAI,EAAE,CAAC;IAClD,KAAK,WAAW;MACZ,OAAO,IAAI3B,SAAS,CAACiB,OAAO,CAACU,OAAO,IAAI,EAAE,EAAE;QACxCC,aAAa,EAAEX,OAAO,CAACW;MAC3B,CAAC,CAAC;IACN,KAAK,QAAQ;MACT,OAAO,IAAIpB,aAAa,CAACS,OAAO,CAACU,OAAO,IAAI,EAAE,CAAC;IACnD;MACI,OAAO,IAAIxB,WAAW,CAACc,OAAO,CAACU,OAAO,IAAI,EAAE,EAAEV,OAAO,CAACC,IAAI,IAAI,SAAS,CAAC;EAChF;AACJ;AACA,SAASW,2BAA2BA;AACpC;AACAC,KAAK,EAAEC,WAAW,EAAE;EAChB,MAAMb,IAAI,GAAGY,KAAK,CAACZ,IAAI,IAAIa,WAAW;EACtC,MAAMJ,OAAO,GAAGG,KAAK,CAACH,OAAO,IAAI,EAAE;EACnC,IAAIK,iBAAiB;EACrB,IAAIF,KAAK,CAACF,aAAa,EAAE;IACrBI,iBAAiB,GAAG;MAChBJ,aAAa,EAAEE,KAAK,CAACF;IACzB,CAAC;EACL,CAAC,MACI;IACDI,iBAAiB,GAAG,CAAC,CAAC;EAC1B;EACA,IAAId,IAAI,KAAK,MAAM,EAAE;IACjB,OAAO,IAAIX,iBAAiB,CAAC;MAAEoB;IAAQ,CAAC,CAAC;EAC7C,CAAC,MACI,IAAIT,IAAI,KAAK,WAAW,EAAE;IAC3B,OAAO,IAAIjB,cAAc,CAAC;MAAE0B,OAAO;MAAEK;IAAkB,CAAC,CAAC;EAC7D,CAAC,MACI,IAAId,IAAI,KAAK,QAAQ,EAAE;IACxB,OAAO,IAAIT,kBAAkB,CAAC;MAAEkB;IAAQ,CAAC,CAAC;EAC9C,CAAC,MACI,IAAIT,IAAI,KAAK,UAAU,EAAE;IAC1B,OAAO,IAAIb,oBAAoB,CAAC;MAC5BsB,OAAO;MACPK,iBAAiB;MACjBC,IAAI,EAAEH,KAAK,CAACG;IAChB,CAAC,CAAC;EACN,CAAC,MACI;IACD,OAAO,IAAI7B,gBAAgB,CAAC;MAAEuB,OAAO;MAAET;IAAK,CAAC,CAAC;EAClD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMgB,UAAU,SAASpB,aAAa,CAAC;EAC1C,OAAOqB,OAAOA,CAAA,EAAG;IACb,OAAO,YAAY;EACvB;EACA,IAAIC,QAAQA,CAAA,EAAG;IACX,OAAO,CACH,GAAG,KAAK,CAACA,QAAQ,EACjB,SAAS,EACT,eAAe,EACf,WAAW,EACX,OAAO,EACP,aAAa,CAChB;EACL;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,YAAY,EAAE,gBAAgB;MAC9BC,iBAAiB,EAAE,sBAAsB;MACzCC,YAAY,EAAE;IAClB,CAAC;EACL;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,SAAS,EAAE,OAAO;MAClBJ,YAAY,EAAE,gBAAgB;MAC9BK,qBAAqB,EAAE,0BAA0B;MACjDJ,iBAAiB,EAAE,sBAAsB;MACzCK,0BAA0B,EAAE,gCAAgC;MAC5DC,4BAA4B,EAAE;IAClC,CAAC;EACL;EACAC,WAAWA,CAACC,MAAM,EAClB;EACAC,aAAa,EAAE;IACX,KAAK,CAACD,MAAM,IAAI,CAAC,CAAC,CAAC;IACnBE,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,kBAAkB,EAAE;MAC5CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,GAAG,EAAE;MAC7BC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,uBAAuB,EAAE;MACjDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,mBAAmB,EAAE;MAC7CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,4BAA4B,EAAE;MACtDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,8BAA8B,EAAE;MACxDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,qBAAqB,EAAE;MAC/CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAAChB,YAAY,GACbS,MAAM,EAAET,YAAY,IAAI1B,sBAAsB,CAAC,gBAAgB,CAAC;IACpE,IAAI,CAAC2B,iBAAiB,GAClBQ,MAAM,EAAER,iBAAiB,IACrB3B,sBAAsB,CAAC,sBAAsB,CAAC;IACtD,IAAI,CAAC,IAAI,CAAC2B,iBAAiB,IAAI,CAAC,IAAI,CAACD,YAAY,EAAE;MAC/C,MAAM,IAAIb,KAAK,CAAC,0CAA0C,CAAC;IAC/D;IACA,IAAI,CAACmB,0BAA0B,GAC3BG,MAAM,EAAEH,0BAA0B,IAC9BhC,sBAAsB,CAAC,gCAAgC,CAAC;IAChE,IAAI,CAACiC,4BAA4B,GAC7BE,MAAM,EAAEF,4BAA4B,IAChCjC,sBAAsB,CAAC,kCAAkC,CAAC;IAClE,IAAI,CAAC+B,qBAAqB,GACtBI,MAAM,EAAEJ,qBAAqB,IACzB/B,sBAAsB,CAAC,0BAA0B,CAAC;IAC1D,IAAI,CAAC2C,mBAAmB,GACpBR,MAAM,EAAEQ,mBAAmB,IACvB3C,sBAAsB,CAAC,wBAAwB,CAAC;IACxD,IAAI,CAAC4B,YAAY,GACbO,MAAM,EAAEC,aAAa,EAAER,YAAY,IAC/B5B,sBAAsB,CAAC,qBAAqB,CAAC;IACrD,IAAI,CAAC8B,SAAS,GAAGK,MAAM,EAAEL,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACc,WAAW,GAAGT,MAAM,EAAES,WAAW,IAAI,CAAC,CAAC;IAC5C,IAAI,CAACC,OAAO,GAAGV,MAAM,EAAEU,OAAO;IAC9B,IAAI,CAACC,WAAW,GAAGX,MAAM,EAAEW,WAAW,IAAI,IAAI,CAACA,WAAW;IAC1D,IAAI,CAACC,IAAI,GAAGZ,MAAM,EAAEY,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,CAACC,gBAAgB,GAAGb,MAAM,EAAEa,gBAAgB,IAAI,IAAI,CAACA,gBAAgB;IACzE,IAAI,CAACC,eAAe,GAAGd,MAAM,EAAEc,eAAe,IAAI,IAAI,CAACA,eAAe;IACtE,IAAI,CAACC,SAAS,GAAGf,MAAM,EAAEe,SAAS;IAClC,IAAI,CAACC,CAAC,GAAGhB,MAAM,EAAEgB,CAAC,IAAI,IAAI,CAACA,CAAC;IAC5B,IAAI,CAACC,SAAS,GAAGjB,MAAM,EAAEiB,SAAS;IAClC,IAAI,CAACC,IAAI,GAAGlB,MAAM,EAAEkB,IAAI;IACxB,IAAI,CAACC,IAAI,GAAGnB,MAAM,EAAEmB,IAAI;IACxB,IAAI,CAACC,SAAS,GAAGpB,MAAM,EAAEoB,SAAS,IAAI,KAAK;IAC3C,IAAI,IAAI,CAAC5B,iBAAiB,EAAE;MACxB,IAAI,CAAC,IAAI,CAACK,0BAA0B,IAAI,CAAC,IAAI,CAACW,mBAAmB,EAAE;QAC/D,MAAM,IAAI9B,KAAK,CAAC,0CAA0C,CAAC;MAC/D;MACA,IAAI,CAAC,IAAI,CAACoB,4BAA4B,EAAE;QACpC,MAAM,IAAIpB,KAAK,CAAC,4CAA4C,CAAC;MACjE;MACA,IAAI,CAAC,IAAI,CAACkB,qBAAqB,EAAE;QAC7B,MAAM,IAAIlB,KAAK,CAAC,oCAAoC,CAAC;MACzD;MACA,IAAI,CAACa,YAAY,GAAG,IAAI,CAACA,YAAY,IAAI,EAAE;IAC/C;IACA,IAAI,CAAC8B,YAAY,GAAG;MAChBC,MAAM,EAAE,IAAI,CAAC/B,YAAY;MACzBE,YAAY,EAAE,IAAI,CAACA,YAAY;MAC/B8B,OAAO,EAAEtB,aAAa,EAAEuB,QAAQ,IAAIxB,MAAM,EAAEC,aAAa,EAAEuB,QAAQ;MACnEC,uBAAuB,EAAE,IAAI;MAC7BC,cAAc,EAAEzB,aAAa,EAAE0B,WAAW,EAAEC,OAAO,IAC/C5B,MAAM,EAAEC,aAAa,EAAE0B,WAAW,EAAEC,OAAO;MAC/CC,YAAY,EAAE5B,aAAa,EAAE0B,WAAW,EAAEG,MAAM,IAC5C9B,MAAM,EAAEC,aAAa,EAAE0B,WAAW,EAAEG,MAAM;MAC9C,GAAG7B,aAAa;MAChB,GAAGD,MAAM,EAAEC;IACf,CAAC;EACL;EACA;AACJ;AACA;EACI8B,gBAAgBA,CAACC,OAAO,EAAE;IACtB,OAAO;MACHC,KAAK,EAAE,IAAI,CAACtC,SAAS;MACrBgB,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BuB,KAAK,EAAE,IAAI,CAACtB,IAAI;MAChBuB,iBAAiB,EAAE,IAAI,CAACtB,gBAAgB;MACxCuB,gBAAgB,EAAE,IAAI,CAACtB,eAAe;MACtCuB,UAAU,EAAE,IAAI,CAACtB,SAAS,KAAK,CAAC,CAAC,GAAGuB,SAAS,GAAG,IAAI,CAACvB,SAAS;MAC9DC,CAAC,EAAE,IAAI,CAACA,CAAC;MACTuB,UAAU,EAAE,IAAI,CAACtB,SAAS;MAC1BC,IAAI,EAAEc,OAAO,EAAEd,IAAI,IAAI,IAAI,CAACA,IAAI;MAChCC,IAAI,EAAE,IAAI,CAACA,IAAI;MACfqB,MAAM,EAAE,IAAI,CAACpB,SAAS;MACtBqB,SAAS,EAAET,OAAO,EAAES,SAAS,KACxBT,OAAO,EAAEU,KAAK,GACTV,OAAO,EAAEU,KAAK,CAACC,GAAG,CAAChF,sBAAsB,CAAC,GAC1C2E,SAAS,CAAC;MACpBzD,aAAa,EAAEmD,OAAO,EAAEnD,aAAa;MACrC,GAAG,IAAI,CAAC4B;IACZ,CAAC;EACL;EACA;EACAmC,kBAAkBA,CAAA,EAAG;IACjB,OAAO;MACHC,UAAU,EAAE,IAAI,CAAClD,SAAS;MAC1B,GAAG,IAAI,CAACoC,gBAAgB,CAAC,CAAC;MAC1B,GAAG,IAAI,CAACV;IACZ,CAAC;EACL;EACA,OAAOyB,qBAAqBA,CAACC,QAAQ,EAAEf,OAAO,EAAEgB,UAAU,EAAE;IACxD,MAAMC,cAAc,GAAGF,QAAQ,CAACJ,GAAG,CAAEzE,OAAO,KAAM;MAC9CC,IAAI,EAAEG,mBAAmB,CAACJ,OAAO,CAAC;MAClCU,OAAO,EAAEV,OAAO,CAACU,OAAO;MACxBM,IAAI,EAAEhB,OAAO,CAACgB,IAAI;MAClBL,aAAa,EAAEX,OAAO,CAACe,iBAAiB,CACnCJ;IACT,CAAC,CAAC,CAAC;IACH,MAAMiD,MAAM,GAAG;MACX,GAAG,IAAI,CAACC,gBAAgB,CAACC,OAAO,CAAC;MACjCe,QAAQ,EAAEE,cAAc;MACxBT,MAAM,EAAE;IACZ,CAAC;IACD,IAAIxD,WAAW;IACf,MAAMkE,cAAc,GAAG,MAAM,IAAI,CAACC,mBAAmB,CAACrB,MAAM,EAAEE,OAAO,CAAC;IACtE,WAAW,MAAMoB,IAAI,IAAIF,cAAc,EAAE;MACrC,MAAMG,MAAM,GAAGD,IAAI,CAACE,OAAO,CAAC,CAAC,CAAC;MAC9B,IAAI,CAACD,MAAM,EAAE;QACT;MACJ;MACA,MAAM;QAAEtE;MAAM,CAAC,GAAGsE,MAAM;MACxB,MAAME,KAAK,GAAGzE,2BAA2B,CAACC,KAAK,EAAEC,WAAW,CAAC;MAC7DA,WAAW,GAAGD,KAAK,CAACZ,IAAI,IAAIa,WAAW;MACvC,MAAMwE,eAAe,GAAG;QACpBC,MAAM,EAAEzB,OAAO,CAAC0B,WAAW,IAAI,CAAC;QAChCC,UAAU,EAAEN,MAAM,CAACO,KAAK,IAAI;MAChC,CAAC;MACD,MAAMC,eAAe,GAAG,IAAI1G,mBAAmB,CAAC;QAC5Ce,OAAO,EAAEqF,KAAK;QACdO,IAAI,EAAEP,KAAK,CAAC3E,OAAO;QACnBmF,cAAc,EAAEP;MACpB,CAAC,CAAC;MACF,MAAMK,eAAe;MACrB;MACA,KAAKb,UAAU,EAAEgB,iBAAiB,CAACH,eAAe,CAACC,IAAI,IAAI,EAAE,EAAEN,eAAe,EAAElB,SAAS,EAAEA,SAAS,EAAEA,SAAS,EAAE;QAAEiB,KAAK,EAAEM;MAAgB,CAAC,CAAC;IAChJ;IACA,IAAI7B,OAAO,CAACiC,MAAM,EAAEC,OAAO,EAAE;MACzB,MAAM,IAAIxF,KAAK,CAAC,YAAY,CAAC;IACjC;EACJ;EACA;AACJ;AACA;EACIyF,iBAAiBA,CAAA,EAAG;IAChB,OAAO,IAAI,CAACvB,kBAAkB,CAAC,CAAC;EACpC;EACA;EACA,MAAMwB,SAASA,CAACrB,QAAQ,EAAEf,OAAO,EAAEgB,UAAU,EAAE;IAC3C,MAAMqB,UAAU,GAAG,CAAC,CAAC;IACrB,MAAMvC,MAAM,GAAG,IAAI,CAACC,gBAAgB,CAACC,OAAO,CAAC;IAC7C,MAAMiB,cAAc,GAAGF,QAAQ,CAACJ,GAAG,CAAEzE,OAAO,KAAM;MAC9CC,IAAI,EAAEG,mBAAmB,CAACJ,OAAO,CAAC;MAClCU,OAAO,EAAEV,OAAO,CAACU,OAAO;MACxBM,IAAI,EAAEhB,OAAO,CAACgB,IAAI;MAClBL,aAAa,EAAEX,OAAO,CAACe,iBAAiB,CACnCJ;IACT,CAAC,CAAC,CAAC;IACH,IAAIiD,MAAM,CAACU,MAAM,EAAE;MACf,MAAMA,MAAM,GAAG,MAAM,IAAI,CAACM,qBAAqB,CAACC,QAAQ,EAAEf,OAAO,EAAEgB,UAAU,CAAC;MAC9E,MAAMsB,WAAW,GAAG,CAAC,CAAC;MACtB,WAAW,MAAMf,KAAK,IAAIf,MAAM,EAAE;QAC9B,MAAMoB,KAAK,GAAGL,KAAK,CAACQ,cAAc,EAAEJ,UAAU,IAAI,CAAC;QACnD,IAAIW,WAAW,CAACV,KAAK,CAAC,KAAKtB,SAAS,EAAE;UAClCgC,WAAW,CAACV,KAAK,CAAC,GAAGL,KAAK;QAC9B,CAAC,MACI;UACDe,WAAW,CAACV,KAAK,CAAC,GAAGU,WAAW,CAACV,KAAK,CAAC,CAACW,MAAM,CAAChB,KAAK,CAAC;QACzD;MACJ;MACA,MAAMiB,WAAW,GAAGtE,MAAM,CAACuE,OAAO,CAACH,WAAW,CAAC,CAC1CI,IAAI,CAAC,CAAC,CAACC,IAAI,CAAC,EAAE,CAACC,IAAI,CAAC,KAAKC,QAAQ,CAACF,IAAI,EAAE,EAAE,CAAC,GAAGE,QAAQ,CAACD,IAAI,EAAE,EAAE,CAAC,CAAC,CACjEjC,GAAG,CAAC,CAAC,CAACmC,CAAC,EAAEvE,KAAK,CAAC,KAAKA,KAAK,CAAC;MAC/B,OAAO;QAAEiE;MAAY,CAAC;IAC1B,CAAC,MACI;MACD,MAAMpB,IAAI,GAAG,MAAM,IAAI,CAACD,mBAAmB,CAAC;QACxC,GAAGrB,MAAM;QACTU,MAAM,EAAE,KAAK;QACbO,QAAQ,EAAEE;MACd,CAAC,EAAE;QACCgB,MAAM,EAAEjC,OAAO,EAAEiC,MAAM;QACvB,GAAGjC,OAAO,EAAEA;MAChB,CAAC,CAAC;MACF,MAAM;QAAE+C,iBAAiB,EAAEC,gBAAgB;QAAEC,aAAa,EAAEC,YAAY;QAAEC,YAAY,EAAEC;MAAa,CAAC,GAAGhC,IAAI,EAAEiC,KAAK,IAAI,CAAC,CAAC;MAC1H,IAAIL,gBAAgB,EAAE;QAClBX,UAAU,CAACW,gBAAgB,GACvB,CAACX,UAAU,CAACW,gBAAgB,IAAI,CAAC,IAAIA,gBAAgB;MAC7D;MACA,IAAIE,YAAY,EAAE;QACdb,UAAU,CAACa,YAAY,GAAG,CAACb,UAAU,CAACa,YAAY,IAAI,CAAC,IAAIA,YAAY;MAC3E;MACA,IAAIE,WAAW,EAAE;QACbf,UAAU,CAACe,WAAW,GAAG,CAACf,UAAU,CAACe,WAAW,IAAI,CAAC,IAAIA,WAAW;MACxE;MACA,MAAMZ,WAAW,GAAG,EAAE;MACtB,KAAK,MAAMc,IAAI,IAAIlC,IAAI,EAAEE,OAAO,IAAI,EAAE,EAAE;QACpC,MAAMQ,IAAI,GAAGwB,IAAI,CAACpH,OAAO,EAAEU,OAAO,IAAI,EAAE;QACxC,MAAM2G,UAAU,GAAG;UACfzB,IAAI;UACJ5F,OAAO,EAAES,2BAA2B,CAAC2G,IAAI,CAACpH,OAAO,IAAI;YAAEC,IAAI,EAAE;UAAY,CAAC;QAC9E,CAAC;QACD,IAAImH,IAAI,CAACE,aAAa,EAAE;UACpBD,UAAU,CAACxB,cAAc,GAAG;YAAEyB,aAAa,EAAEF,IAAI,CAACE;UAAc,CAAC;QACrE;QACAhB,WAAW,CAACiB,IAAI,CAACF,UAAU,CAAC;MAChC;MACA,OAAO;QACHf,WAAW;QACXkB,SAAS,EAAE;UAAErB;QAAW;MAC5B,CAAC;IACL;EACJ;EACA,MAAMsB,wBAAwBA,CAAC5C,QAAQ,EAAE;IACrC,IAAI6C,UAAU,GAAG,CAAC;IAClB,IAAIC,gBAAgB,GAAG,CAAC;IACxB,IAAIC,aAAa,GAAG,CAAC;IACrB;IACA,IAAI9I,uBAAuB,CAAC,IAAI,CAAC2C,SAAS,CAAC,KAAK,eAAe,EAAE;MAC7DkG,gBAAgB,GAAG,CAAC;MACpBC,aAAa,GAAG,CAAC,CAAC;IACtB,CAAC,MACI,IAAI9I,uBAAuB,CAAC,IAAI,CAAC2C,SAAS,CAAC,CAACoG,UAAU,CAAC,OAAO,CAAC,EAAE;MAClEF,gBAAgB,GAAG,CAAC;MACpBC,aAAa,GAAG,CAAC;IACrB;IACA,MAAME,eAAe,GAAG,MAAMC,OAAO,CAACC,GAAG,CAACnD,QAAQ,CAACJ,GAAG,CAAC,MAAOzE,OAAO,IAAK;MACtE,MAAMiI,SAAS,GAAG,MAAM,IAAI,CAACC,YAAY,CAAClI,OAAO,CAACU,OAAO,CAAC;MAC1D,MAAMyH,SAAS,GAAG,MAAM,IAAI,CAACD,YAAY,CAAC9H,mBAAmB,CAACJ,OAAO,CAAC,CAAC;MACvE,MAAMoI,SAAS,GAAGpI,OAAO,CAACgB,IAAI,KAAKoD,SAAS,GACtCwD,aAAa,IAAI,MAAM,IAAI,CAACM,YAAY,CAAClI,OAAO,CAACgB,IAAI,CAAC,CAAC,GACvD,CAAC;MACP,MAAMqH,KAAK,GAAGJ,SAAS,GAAGN,gBAAgB,GAAGQ,SAAS,GAAGC,SAAS;MAClEV,UAAU,IAAIW,KAAK;MACnB,OAAOA,KAAK;IAChB,CAAC,CAAC,CAAC;IACHX,UAAU,IAAI,CAAC,CAAC,CAAC;IACjB,OAAO;MAAEA,UAAU;MAAEI;IAAgB,CAAC;EAC1C;EACA,MAAM7C,mBAAmBA,CAACqD,OAAO,EAAExE,OAAO,EAAE;IACxC,MAAMyE,cAAc,GAAG,IAAI,CAACC,iBAAiB,CAAC1E,OAAO,CAAC;IACtD,OAAO,IAAI,CAAC2E,MAAM,CAACC,IAAI,CAAC,YAAY;MAChC,IAAI;QACA,MAAMC,GAAG,GAAG,MAAM,IAAI,CAACC,MAAM,CAACC,IAAI,CAACC,WAAW,CAACC,MAAM,CAACT,OAAO,EAAEC,cAAc,CAAC;QAC9E,OAAOI,GAAG;MACd,CAAC,CACD,OAAOK,CAAC,EAAE;QACN,MAAMC,KAAK,GAAGnJ,qBAAqB,CAACkJ,CAAC,CAAC;QACtC,MAAMC,KAAK;MACf;IACJ,CAAC,CAAC;EACN;EACAT,iBAAiBA,CAAC1E,OAAO,EAAE;IACvB,IAAI,CAAC,IAAI,CAAC8E,MAAM,EAAE;MACd,MAAMM,oBAAoB,GAAG;QACzBtH,4BAA4B,EAAE,IAAI,CAACA,4BAA4B;QAC/DD,0BAA0B,EAAE,IAAI,CAACA,0BAA0B;QAC3DL,iBAAiB,EAAE,IAAI,CAACA,iBAAiB;QACzCgB,mBAAmB,EAAE,IAAI,CAACA,mBAAmB;QAC7Ce,OAAO,EAAE,IAAI,CAACF,YAAY,CAACE;MAC/B,CAAC;MACD,MAAM8F,QAAQ,GAAGzJ,WAAW,CAACwJ,oBAAoB,CAAC;MAClD,MAAMtF,MAAM,GAAG;QACX,GAAG,IAAI,CAACT,YAAY;QACpBE,OAAO,EAAE8F,QAAQ;QACjB3G,OAAO,EAAE,IAAI,CAACA,OAAO;QACrB4G,UAAU,EAAE;MAChB,CAAC;MACD,IAAI,CAACxF,MAAM,CAACP,OAAO,EAAE;QACjB,OAAOO,MAAM,CAACP,OAAO;MACzB;MACA,IAAI,CAACuF,MAAM,GAAG,IAAI/J,YAAY,CAAC+E,MAAM,CAAC;IAC1C;IACA,MAAM2E,cAAc,GAAG;MACnB,GAAG,IAAI,CAACpF,YAAY;MACpB,GAAGW;IACP,CAAC;IACD,IAAI,IAAI,CAACxC,iBAAiB,EAAE;MACxBiH,cAAc,CAAC7E,OAAO,GAAG;QACrB,SAAS,EAAE,IAAI,CAACpC,iBAAiB;QACjC,GAAGiH,cAAc,CAAC7E;MACtB,CAAC;MACD6E,cAAc,CAACc,KAAK,GAAG;QACnB,aAAa,EAAE,IAAI,CAAC3H,qBAAqB;QACzC,GAAG6G,cAAc,CAACc;MACtB,CAAC;IACL;IACA,OAAOd,cAAc;EACzB;EACAe,QAAQA,CAAA,EAAG;IACP,OAAO,QAAQ;EACnB;EACA;EACAC,iBAAiBA,CAAC,GAAGC,UAAU,EAAE;IAC7B,OAAOA,UAAU,CAACC,MAAM,CAAC,CAACC,GAAG,EAAElC,SAAS,KAAK;MACzC,IAAIA,SAAS,IAAIA,SAAS,CAACrB,UAAU,EAAE;QACnCuD,GAAG,CAACvD,UAAU,CAACW,gBAAgB,IAC3BU,SAAS,CAACrB,UAAU,CAACW,gBAAgB,IAAI,CAAC;QAC9C4C,GAAG,CAACvD,UAAU,CAACa,YAAY,IAAIQ,SAAS,CAACrB,UAAU,CAACa,YAAY,IAAI,CAAC;QACrE0C,GAAG,CAACvD,UAAU,CAACe,WAAW,IAAIM,SAAS,CAACrB,UAAU,CAACe,WAAW,IAAI,CAAC;MACvE;MACA,OAAOwC,GAAG;IACd,CAAC,EAAE;MACCvD,UAAU,EAAE;QACRW,gBAAgB,EAAE,CAAC;QACnBE,YAAY,EAAE,CAAC;QACfE,WAAW,EAAE;MACjB;IACJ,CAAC,CAAC;EACN;AACJ;AACA,OAAO,MAAMyC,qBAAqB,SAAS1I,UAAU,CAAC;EAClDY,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACbE,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,mBAAmB,EAAE;MAC7CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,qBAAqB,EAAE;MAC/CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACuH,iBAAiB,GAClB9H,MAAM,EAAE8H,iBAAiB,KACpB,OAAOC,OAAO,KAAK,WAAW;IACzB;IACEA,OAAO,CAACC,GAAG,EAAEC,mBAAmB,GAClC3F,SAAS,CAAC;IACxB,IAAI,CAAC4F,MAAM,GAAGlI,MAAM,EAAEkI,MAAM,IAAI,EAAE;IAClC,IAAI,CAACC,mBAAmB,GAAGnI,MAAM,EAAEmI,mBAAmB,IAAI,KAAK;EACnE;EACA,MAAM/D,SAASA,CAACrB,QAAQ,EAAEf,OAAO,EAAEgB,UAAU,EAAE;IAC3C,MAAMoF,gBAAgB,GAAGC,IAAI,CAACC,GAAG,CAAC,CAAC;IACnC,IAAIC,aAAa;IACjB,IAAIC,KAAK,CAACC,OAAO,CAACzG,OAAO,CAAC,EAAE;MACxBuG,aAAa,GAAG;QAAErH,IAAI,EAAEc;MAAQ,CAAC;IACrC,CAAC,MACI,IAAIA,OAAO,EAAEtB,OAAO,IAAI,CAACsB,OAAO,CAACiC,MAAM,EAAE;MAC1CsE,aAAa,GAAG;QACZ,GAAGvG,OAAO;QACViC,MAAM,EAAEyE,WAAW,CAAChI,OAAO,CAACsB,OAAO,CAACtB,OAAO;MAC/C,CAAC;IACL,CAAC,MACI;MACD6H,aAAa,GAAGvG,OAAO,IAAI,CAAC,CAAC;IACjC;IACA,MAAM2G,kBAAkB,GAAG,MAAM,KAAK,CAACvE,SAAS,CAACrB,QAAQ,EAAEwF,aAAa,EAAEvF,UAAU,CAAC;IACrF,MAAM4F,cAAc,GAAGP,IAAI,CAACC,GAAG,CAAC,CAAC;IACjC,MAAMO,qBAAqB,GAAI3K,OAAO,IAAK;MACvC;MACA,IAAI4K,WAAW;MACf,IAAI5K,OAAO,CAACM,QAAQ,CAAC,CAAC,KAAK,OAAO,EAAE;QAChCsK,WAAW,GAAG;UAAE3K,IAAI,EAAE,MAAM;UAAES,OAAO,EAAEV,OAAO,CAACU;QAAQ,CAAC;MAC5D,CAAC,MACI,IAAIV,OAAO,CAACM,QAAQ,CAAC,CAAC,KAAK,IAAI,EAAE;QAClCsK,WAAW,GAAG;UAAE3K,IAAI,EAAE,WAAW;UAAES,OAAO,EAAEV,OAAO,CAACU;QAAQ,CAAC;MACjE,CAAC,MACI,IAAIV,OAAO,CAACM,QAAQ,CAAC,CAAC,KAAK,UAAU,EAAE;QACxCsK,WAAW,GAAG;UAAE3K,IAAI,EAAE,WAAW;UAAES,OAAO,EAAEV,OAAO,CAACU;QAAQ,CAAC;MACjE,CAAC,MACI,IAAIV,OAAO,CAACM,QAAQ,CAAC,CAAC,KAAK,QAAQ,EAAE;QACtCsK,WAAW,GAAG;UAAE3K,IAAI,EAAE,QAAQ;UAAES,OAAO,EAAEV,OAAO,CAACU;QAAQ,CAAC;MAC9D,CAAC,MACI,IAAIV,OAAO,CAACM,QAAQ,CAAC,CAAC,KAAK,SAAS,EAAE;QACvCsK,WAAW,GAAG;UACV3K,IAAI,EAAED,OAAO,CAACC,IAAI;UAClBS,OAAO,EAAEV,OAAO,CAACU;QACrB,CAAC;MACL,CAAC,MACI;QACD,MAAM,IAAIF,KAAK,CAAE,oBAAmBR,OAAQ,EAAC,CAAC;MAClD;MACA,OAAO4K,WAAW;IACtB,CAAC;IACD,MAAMC,mBAAmB,GAAGA,CAAChG,QAAQ,EAAEiG,WAAW,KAAK;MACnD,MAAMlH,MAAM,GAAG;QACX,GAAG,IAAI,CAACC,gBAAgB,CAAC,CAAC;QAC1BE,KAAK,EAAE,IAAI,CAACtC;MAChB,CAAC;MACD,IAAIqJ,WAAW,EAAE9H,IAAI,EAAE;QACnB,IAAIhB,MAAM,CAAC+I,IAAI,CAACnH,MAAM,CAAC,CAACoH,QAAQ,CAAC,MAAM,CAAC,EAAE;UACtC,MAAM,IAAIxK,KAAK,CAAC,oDAAoD,CAAC;QACzE;MACJ;MACA,MAAMyK,YAAY,GAAGpG,QAAQ,CAACJ,GAAG,CAAEzE,OAAO,IAAK2K,qBAAqB,CAAC3K,OAAO,CAAC,CAAC;MAC9E,OAAOiL,YAAY;IACvB,CAAC;IACD,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGT,kBAAkB,CAACnE,WAAW,CAAC6E,MAAM,EAAED,CAAC,IAAI,CAAC,EAAE;MAC/D,MAAM7D,UAAU,GAAGoD,kBAAkB,CAACnE,WAAW,CAAC4E,CAAC,CAAC;MACpD,MAAMD,YAAY,GAAGJ,mBAAmB,CAAChG,QAAQ,EAAEwF,aAAa,CAAC;MACjE,IAAIe,oBAAoB;MACxB,MAAMC,UAAU,GAAG,CACf;QACI3K,OAAO,EAAE2G,UAAU,CAACzB,IAAI;QACxB3F,IAAI,EAAEG,mBAAmB,CAACiH,UAAU,CAACrH,OAAO;MAChD,CAAC,CACJ;MACD,MAAMsL,mBAAmB,GAAG,MAAM1L,uBAAuB,CAAC,IAAI,CAAC6I,MAAM,EAAE,iCAAiC,EAAE;QAAE,GAAG,IAAI,CAAC/D,kBAAkB,CAAC,CAAC;QAAEG,QAAQ,EAAEoG,YAAY;QAAE3G,MAAM,EAAE;MAAM,CAAC,EAAE,IAAI,CAAC0F,MAAM,EAAEqB,UAAU,EAAEnB,gBAAgB,EAAEQ,cAAc,EAAE,IAAI,CAACd,iBAAiB,CAAC;MACrQ,IAAI,IAAI,CAACK,mBAAmB,KAAK,IAAI,EAAE;QACnC,IAAIqB,mBAAmB,CAACC,OAAO,KAAK,IAAI,EAAE;UACtCH,oBAAoB,GAAGE,mBAAmB,CAACE,UAAU;QACzD;QACA,IAAI,CAACnE,UAAU,CAACxB,cAAc,IAC1B,OAAOwB,UAAU,CAACxB,cAAc,KAAK,QAAQ,EAAE;UAC/CwB,UAAU,CAACxB,cAAc,GAAG,CAAC,CAAC;QAClC;QACAwB,UAAU,CAACxB,cAAc,CAACuF,oBAAoB,GAAGA,oBAAoB;MACzE;IACJ;IACA,OAAOX,kBAAkB;EAC7B;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}