{"ast":null,"code":"import { AsyncCaller } from \"../util/async_caller.js\";\nimport { getModelNameForTiktoken } from \"./count_tokens.js\";\nimport { encodingForModel } from \"../util/tiktoken.js\";\nimport { Runnable } from \"../schema/runnable.js\";\nimport { StringPromptValue } from \"../prompts/base.js\";\nimport { ChatPromptValue } from \"../prompts/chat.js\";\nconst getVerbosity = () => false;\n/**\n * Base class for language models, chains, tools.\n */\nexport class BaseLangChain extends Runnable {\n  get lc_attributes() {\n    return {\n      callbacks: undefined,\n      verbose: undefined\n    };\n  }\n  constructor(params) {\n    super(params);\n    /**\n     * Whether to print out response text.\n     */\n    Object.defineProperty(this, \"verbose\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"callbacks\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"tags\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"metadata\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.verbose = params.verbose ?? getVerbosity();\n    this.callbacks = params.callbacks;\n    this.tags = params.tags ?? [];\n    this.metadata = params.metadata ?? {};\n  }\n}\n/**\n * Base class for language models.\n */\nexport class BaseLanguageModel extends BaseLangChain {\n  /**\n   * Keys that the language model accepts as call options.\n   */\n  get callKeys() {\n    return [\"stop\", \"timeout\", \"signal\", \"tags\", \"metadata\", \"callbacks\"];\n  }\n  constructor({\n    callbacks,\n    callbackManager,\n    ...params\n  }) {\n    super({\n      callbacks: callbacks ?? callbackManager,\n      ...params\n    });\n    /**\n     * The async caller should be used by subclasses to make any async calls,\n     * which will thus benefit from the concurrency and retry logic.\n     */\n    Object.defineProperty(this, \"caller\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"_encoding\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.caller = new AsyncCaller(params ?? {});\n  }\n  async getNumTokens(text) {\n    // fallback to approximate calculation if tiktoken is not available\n    let numTokens = Math.ceil(text.length / 4);\n    if (!this._encoding) {\n      try {\n        this._encoding = await encodingForModel(\"modelName\" in this ? getModelNameForTiktoken(this.modelName) : \"gpt2\");\n      } catch (error) {\n        console.warn(\"Failed to calculate number of tokens, falling back to approximate count\", error);\n      }\n    }\n    if (this._encoding) {\n      numTokens = this._encoding.encode(text).length;\n    }\n    return numTokens;\n  }\n  static _convertInputToPromptValue(input) {\n    if (typeof input === \"string\") {\n      return new StringPromptValue(input);\n    } else if (Array.isArray(input)) {\n      return new ChatPromptValue(input);\n    } else {\n      return input;\n    }\n  }\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams() {\n    return {};\n  }\n  /**\n   * @deprecated\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this._identifyingParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  /**\n   * @deprecated\n   * Load an LLM from a json-like object describing it.\n   */\n  static async deserialize(data) {\n    const {\n      _type,\n      _model,\n      ...rest\n    } = data;\n    if (_model && _model !== \"base_chat_model\") {\n      throw new Error(`Cannot load LLM with model ${_model}`);\n    }\n    const Cls = {\n      openai: (await import(\"../chat_models/openai.js\")).ChatOpenAI\n    }[_type];\n    if (Cls === undefined) {\n      throw new Error(`Cannot load LLM with type ${_type}`);\n    }\n    return new Cls(rest);\n  }\n}\n/*\n * Calculate max tokens for given model and prompt.\n * That is the model size - number of tokens in prompt.\n */\nexport { calculateMaxTokens } from \"./count_tokens.js\";","map":{"version":3,"names":["AsyncCaller","getModelNameForTiktoken","encodingForModel","Runnable","StringPromptValue","ChatPromptValue","getVerbosity","BaseLangChain","lc_attributes","callbacks","undefined","verbose","constructor","params","Object","defineProperty","enumerable","configurable","writable","value","tags","metadata","BaseLanguageModel","callKeys","callbackManager","caller","getNumTokens","text","numTokens","Math","ceil","length","_encoding","modelName","error","console","warn","encode","_convertInputToPromptValue","input","Array","isArray","_identifyingParams","serialize","_type","_llmType","_model","_modelType","deserialize","data","rest","Error","Cls","openai","ChatOpenAI","calculateMaxTokens"],"sources":["/Users/mayamagavi/instalily/case-study/node_modules/langchain/dist/base_language/index.js"],"sourcesContent":["import { AsyncCaller } from \"../util/async_caller.js\";\nimport { getModelNameForTiktoken } from \"./count_tokens.js\";\nimport { encodingForModel } from \"../util/tiktoken.js\";\nimport { Runnable } from \"../schema/runnable.js\";\nimport { StringPromptValue } from \"../prompts/base.js\";\nimport { ChatPromptValue } from \"../prompts/chat.js\";\nconst getVerbosity = () => false;\n/**\n * Base class for language models, chains, tools.\n */\nexport class BaseLangChain extends Runnable {\n    get lc_attributes() {\n        return {\n            callbacks: undefined,\n            verbose: undefined,\n        };\n    }\n    constructor(params) {\n        super(params);\n        /**\n         * Whether to print out response text.\n         */\n        Object.defineProperty(this, \"verbose\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"callbacks\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"tags\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"metadata\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.verbose = params.verbose ?? getVerbosity();\n        this.callbacks = params.callbacks;\n        this.tags = params.tags ?? [];\n        this.metadata = params.metadata ?? {};\n    }\n}\n/**\n * Base class for language models.\n */\nexport class BaseLanguageModel extends BaseLangChain {\n    /**\n     * Keys that the language model accepts as call options.\n     */\n    get callKeys() {\n        return [\"stop\", \"timeout\", \"signal\", \"tags\", \"metadata\", \"callbacks\"];\n    }\n    constructor({ callbacks, callbackManager, ...params }) {\n        super({\n            callbacks: callbacks ?? callbackManager,\n            ...params,\n        });\n        /**\n         * The async caller should be used by subclasses to make any async calls,\n         * which will thus benefit from the concurrency and retry logic.\n         */\n        Object.defineProperty(this, \"caller\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"_encoding\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.caller = new AsyncCaller(params ?? {});\n    }\n    async getNumTokens(text) {\n        // fallback to approximate calculation if tiktoken is not available\n        let numTokens = Math.ceil(text.length / 4);\n        if (!this._encoding) {\n            try {\n                this._encoding = await encodingForModel(\"modelName\" in this\n                    ? getModelNameForTiktoken(this.modelName)\n                    : \"gpt2\");\n            }\n            catch (error) {\n                console.warn(\"Failed to calculate number of tokens, falling back to approximate count\", error);\n            }\n        }\n        if (this._encoding) {\n            numTokens = this._encoding.encode(text).length;\n        }\n        return numTokens;\n    }\n    static _convertInputToPromptValue(input) {\n        if (typeof input === \"string\") {\n            return new StringPromptValue(input);\n        }\n        else if (Array.isArray(input)) {\n            return new ChatPromptValue(input);\n        }\n        else {\n            return input;\n        }\n    }\n    /**\n     * Get the identifying parameters of the LLM.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _identifyingParams() {\n        return {};\n    }\n    /**\n     * @deprecated\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this._identifyingParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    /**\n     * @deprecated\n     * Load an LLM from a json-like object describing it.\n     */\n    static async deserialize(data) {\n        const { _type, _model, ...rest } = data;\n        if (_model && _model !== \"base_chat_model\") {\n            throw new Error(`Cannot load LLM with model ${_model}`);\n        }\n        const Cls = {\n            openai: (await import(\"../chat_models/openai.js\")).ChatOpenAI,\n        }[_type];\n        if (Cls === undefined) {\n            throw new Error(`Cannot load LLM with type ${_type}`);\n        }\n        return new Cls(rest);\n    }\n}\n/*\n * Calculate max tokens for given model and prompt.\n * That is the model size - number of tokens in prompt.\n */\nexport { calculateMaxTokens } from \"./count_tokens.js\";\n"],"mappings":"AAAA,SAASA,WAAW,QAAQ,yBAAyB;AACrD,SAASC,uBAAuB,QAAQ,mBAAmB;AAC3D,SAASC,gBAAgB,QAAQ,qBAAqB;AACtD,SAASC,QAAQ,QAAQ,uBAAuB;AAChD,SAASC,iBAAiB,QAAQ,oBAAoB;AACtD,SAASC,eAAe,QAAQ,oBAAoB;AACpD,MAAMC,YAAY,GAAGA,CAAA,KAAM,KAAK;AAChC;AACA;AACA;AACA,OAAO,MAAMC,aAAa,SAASJ,QAAQ,CAAC;EACxC,IAAIK,aAAaA,CAAA,EAAG;IAChB,OAAO;MACHC,SAAS,EAAEC,SAAS;MACpBC,OAAO,EAAED;IACb,CAAC;EACL;EACAE,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb;AACR;AACA;IACQC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,UAAU,EAAE;MACpCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACR,OAAO,GAAGE,MAAM,CAACF,OAAO,IAAIL,YAAY,CAAC,CAAC;IAC/C,IAAI,CAACG,SAAS,GAAGI,MAAM,CAACJ,SAAS;IACjC,IAAI,CAACW,IAAI,GAAGP,MAAM,CAACO,IAAI,IAAI,EAAE;IAC7B,IAAI,CAACC,QAAQ,GAAGR,MAAM,CAACQ,QAAQ,IAAI,CAAC,CAAC;EACzC;AACJ;AACA;AACA;AACA;AACA,OAAO,MAAMC,iBAAiB,SAASf,aAAa,CAAC;EACjD;AACJ;AACA;EACI,IAAIgB,QAAQA,CAAA,EAAG;IACX,OAAO,CAAC,MAAM,EAAE,SAAS,EAAE,QAAQ,EAAE,MAAM,EAAE,UAAU,EAAE,WAAW,CAAC;EACzE;EACAX,WAAWA,CAAC;IAAEH,SAAS;IAAEe,eAAe;IAAE,GAAGX;EAAO,CAAC,EAAE;IACnD,KAAK,CAAC;MACFJ,SAAS,EAAEA,SAAS,IAAIe,eAAe;MACvC,GAAGX;IACP,CAAC,CAAC;IACF;AACR;AACA;AACA;IACQC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACM,MAAM,GAAG,IAAIzB,WAAW,CAACa,MAAM,IAAI,CAAC,CAAC,CAAC;EAC/C;EACA,MAAMa,YAAYA,CAACC,IAAI,EAAE;IACrB;IACA,IAAIC,SAAS,GAAGC,IAAI,CAACC,IAAI,CAACH,IAAI,CAACI,MAAM,GAAG,CAAC,CAAC;IAC1C,IAAI,CAAC,IAAI,CAACC,SAAS,EAAE;MACjB,IAAI;QACA,IAAI,CAACA,SAAS,GAAG,MAAM9B,gBAAgB,CAAC,WAAW,IAAI,IAAI,GACrDD,uBAAuB,CAAC,IAAI,CAACgC,SAAS,CAAC,GACvC,MAAM,CAAC;MACjB,CAAC,CACD,OAAOC,KAAK,EAAE;QACVC,OAAO,CAACC,IAAI,CAAC,yEAAyE,EAAEF,KAAK,CAAC;MAClG;IACJ;IACA,IAAI,IAAI,CAACF,SAAS,EAAE;MAChBJ,SAAS,GAAG,IAAI,CAACI,SAAS,CAACK,MAAM,CAACV,IAAI,CAAC,CAACI,MAAM;IAClD;IACA,OAAOH,SAAS;EACpB;EACA,OAAOU,0BAA0BA,CAACC,KAAK,EAAE;IACrC,IAAI,OAAOA,KAAK,KAAK,QAAQ,EAAE;MAC3B,OAAO,IAAInC,iBAAiB,CAACmC,KAAK,CAAC;IACvC,CAAC,MACI,IAAIC,KAAK,CAACC,OAAO,CAACF,KAAK,CAAC,EAAE;MAC3B,OAAO,IAAIlC,eAAe,CAACkC,KAAK,CAAC;IACrC,CAAC,MACI;MACD,OAAOA,KAAK;IAChB;EACJ;EACA;AACJ;AACA;EACI;EACAG,kBAAkBA,CAAA,EAAG;IACjB,OAAO,CAAC,CAAC;EACb;EACA;AACJ;AACA;AACA;EACIC,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACD,kBAAkB,CAAC,CAAC;MAC5BE,KAAK,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;MACtBC,MAAM,EAAE,IAAI,CAACC,UAAU,CAAC;IAC5B,CAAC;EACL;EACA;AACJ;AACA;AACA;EACI,aAAaC,WAAWA,CAACC,IAAI,EAAE;IAC3B,MAAM;MAAEL,KAAK;MAAEE,MAAM;MAAE,GAAGI;IAAK,CAAC,GAAGD,IAAI;IACvC,IAAIH,MAAM,IAAIA,MAAM,KAAK,iBAAiB,EAAE;MACxC,MAAM,IAAIK,KAAK,CAAE,8BAA6BL,MAAO,EAAC,CAAC;IAC3D;IACA,MAAMM,GAAG,GAAG;MACRC,MAAM,EAAE,CAAC,MAAM,MAAM,CAAC,0BAA0B,CAAC,EAAEC;IACvD,CAAC,CAACV,KAAK,CAAC;IACR,IAAIQ,GAAG,KAAK1C,SAAS,EAAE;MACnB,MAAM,IAAIyC,KAAK,CAAE,6BAA4BP,KAAM,EAAC,CAAC;IACzD;IACA,OAAO,IAAIQ,GAAG,CAACF,IAAI,CAAC;EACxB;AACJ;AACA;AACA;AACA;AACA;AACA,SAASK,kBAAkB,QAAQ,mBAAmB"},"metadata":{},"sourceType":"module","externalDependencies":[]}