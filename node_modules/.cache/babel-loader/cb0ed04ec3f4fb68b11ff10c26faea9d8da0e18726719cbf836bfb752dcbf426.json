{"ast":null,"code":"import { AIMessage, HumanMessage, RUN_KEY } from \"../schema/index.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\n/**\n * Creates a transform stream for encoding chat message chunks.\n * @deprecated Use {@link BytesOutputParser} instead\n * @returns A TransformStream instance that encodes chat message chunks.\n */\nexport function createChatMessageChunkEncoderStream() {\n  const textEncoder = new TextEncoder();\n  return new TransformStream({\n    transform(chunk, controller) {\n      controller.enqueue(textEncoder.encode(chunk.content));\n    }\n  });\n}\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport class BaseChatModel extends BaseLanguageModel {\n  constructor(fields) {\n    super(fields);\n    Object.defineProperty(this, \"lc_namespace\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: [\"langchain\", \"chat_models\", this._llmType()]\n    });\n  }\n  _separateRunnableConfigFromCallOptions(options) {\n    const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n    if (callOptions?.timeout && !callOptions.signal) {\n      callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n    }\n    return [runnableConfig, callOptions];\n  }\n  /**\n   * Invokes the chat model with a single input.\n   * @param input The input for the language model.\n   * @param options The call options.\n   * @returns A Promise that resolves to a BaseMessageChunk.\n   */\n  async invoke(input, options) {\n    const promptValue = BaseChatModel._convertInputToPromptValue(input);\n    const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n    const chatGeneration = result.generations[0][0];\n    // TODO: Remove cast after figuring out inheritance\n    return chatGeneration.message;\n  }\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(_messages, _options, _runManager) {\n    throw new Error(\"Not implemented.\");\n  }\n  async *_streamIterator(input, options) {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (this._streamResponseChunks === BaseChatModel.prototype._streamResponseChunks) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseChatModel._convertInputToPromptValue(input);\n      const messages = prompt.toChatMessages();\n      const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n      const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions)\n      };\n      const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), [messages], undefined, undefined, extra);\n      let generationChunk;\n      try {\n        for await (const chunk of this._streamResponseChunks(messages, callOptions, runManagers?.[0])) {\n          yield chunk.message;\n          if (!generationChunk) {\n            generationChunk = chunk;\n          } else {\n            generationChunk = generationChunk.concat(chunk);\n          }\n        }\n      } catch (err) {\n        await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMError(err)));\n        throw err;\n      }\n      await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMEnd({\n        // TODO: Remove cast after figuring out inheritance\n        generations: [[generationChunk]]\n      })));\n    }\n  }\n  /**\n   * Generates chat based on the input messages.\n   * @param messages An array of arrays of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generate(messages, options, callbacks) {\n    // parse call options\n    let parsedOptions;\n    if (Array.isArray(options)) {\n      parsedOptions = {\n        stop: options\n      };\n    } else {\n      parsedOptions = options;\n    }\n    const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n    // create callback manager and start run\n    const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks ?? callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: callOptions,\n      invocation_params: this?.invocationParams(parsedOptions)\n    };\n    const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), messages, undefined, undefined, extra);\n    // generate results\n    const results = await Promise.allSettled(messages.map((messageList, i) => this._generate(messageList, {\n      ...callOptions,\n      promptIndex: i\n    }, runManagers?.[i])));\n    // handle results\n    const generations = [];\n    const llmOutputs = [];\n    await Promise.all(results.map(async (pResult, i) => {\n      if (pResult.status === \"fulfilled\") {\n        const result = pResult.value;\n        generations[i] = result.generations;\n        llmOutputs[i] = result.llmOutput;\n        return runManagers?.[i]?.handleLLMEnd({\n          generations: [result.generations],\n          llmOutput: result.llmOutput\n        });\n      } else {\n        // status === \"rejected\"\n        await runManagers?.[i]?.handleLLMError(pResult.reason);\n        return Promise.reject(pResult.reason);\n      }\n    }));\n    // create combined output\n    const output = {\n      generations,\n      llmOutput: llmOutputs.length ? this._combineLLMOutput?.(...llmOutputs) : undefined\n    };\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers?.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options) {\n    return {};\n  }\n  _modelType() {\n    return \"base_chat_model\";\n  }\n  /**\n   * Generates a prompt based on the input prompt values.\n   * @param promptValues An array of BasePromptValue instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generatePrompt(promptValues, options, callbacks) {\n    const promptMessages = promptValues.map(promptValue => promptValue.toChatMessages());\n    return this.generate(promptMessages, options, callbacks);\n  }\n  /**\n   * Makes a single call to the chat model.\n   * @param messages An array of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async call(messages, options, callbacks) {\n    const result = await this.generate([messages], options, callbacks);\n    const generations = result.generations;\n    return generations[0][0].message;\n  }\n  /**\n   * Makes a single call to the chat model with a prompt value.\n   * @param promptValue The value of the prompt.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async callPrompt(promptValue, options, callbacks) {\n    const promptMessages = promptValue.toChatMessages();\n    return this.call(promptMessages, options, callbacks);\n  }\n  /**\n   * Predicts the next message based on the input messages.\n   * @param messages An array of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async predictMessages(messages, options, callbacks) {\n    return this.call(messages, options, callbacks);\n  }\n  /**\n   * Predicts the next message based on a text input.\n   * @param text The text input.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a string.\n   */\n  async predict(text, options, callbacks) {\n    const message = new HumanMessage(text);\n    const result = await this.call([message], options, callbacks);\n    return result.content;\n  }\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport class SimpleChatModel extends BaseChatModel {\n  async _generate(messages, options, runManager) {\n    const text = await this._call(messages, options, runManager);\n    const message = new AIMessage(text);\n    return {\n      generations: [{\n        text: message.content,\n        message\n      }]\n    };\n  }\n}","map":{"version":3,"names":["AIMessage","HumanMessage","RUN_KEY","BaseLanguageModel","CallbackManager","createChatMessageChunkEncoderStream","textEncoder","TextEncoder","TransformStream","transform","chunk","controller","enqueue","encode","content","BaseChatModel","constructor","fields","Object","defineProperty","enumerable","configurable","writable","value","_llmType","_separateRunnableConfigFromCallOptions","options","runnableConfig","callOptions","timeout","signal","AbortSignal","invoke","input","promptValue","_convertInputToPromptValue","result","generatePrompt","callbacks","chatGeneration","generations","message","_streamResponseChunks","_messages","_options","_runManager","Error","_streamIterator","prototype","prompt","messages","toChatMessages","callbackManager_","configure","tags","metadata","verbose","extra","invocation_params","invocationParams","runManagers","handleChatModelStart","toJSON","undefined","generationChunk","concat","err","Promise","all","map","runManager","handleLLMError","handleLLMEnd","generate","parsedOptions","Array","isArray","stop","results","allSettled","messageList","i","_generate","promptIndex","llmOutputs","pResult","status","llmOutput","reason","reject","output","length","_combineLLMOutput","runIds","manager","runId","_modelType","promptValues","promptMessages","call","callPrompt","predictMessages","predict","text","SimpleChatModel","_call"],"sources":["/Users/mayamagavi/instalily/case-study/node_modules/langchain/dist/chat_models/base.js"],"sourcesContent":["import { AIMessage, HumanMessage, RUN_KEY, } from \"../schema/index.js\";\nimport { BaseLanguageModel, } from \"../base_language/index.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\n/**\n * Creates a transform stream for encoding chat message chunks.\n * @deprecated Use {@link BytesOutputParser} instead\n * @returns A TransformStream instance that encodes chat message chunks.\n */\nexport function createChatMessageChunkEncoderStream() {\n    const textEncoder = new TextEncoder();\n    return new TransformStream({\n        transform(chunk, controller) {\n            controller.enqueue(textEncoder.encode(chunk.content));\n        },\n    });\n}\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport class BaseChatModel extends BaseLanguageModel {\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"chat_models\", this._llmType()]\n        });\n    }\n    _separateRunnableConfigFromCallOptions(options) {\n        const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n        if (callOptions?.timeout && !callOptions.signal) {\n            callOptions.signal = AbortSignal.timeout(callOptions.timeout);\n        }\n        return [runnableConfig, callOptions];\n    }\n    /**\n     * Invokes the chat model with a single input.\n     * @param input The input for the language model.\n     * @param options The call options.\n     * @returns A Promise that resolves to a BaseMessageChunk.\n     */\n    async invoke(input, options) {\n        const promptValue = BaseChatModel._convertInputToPromptValue(input);\n        const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n        const chatGeneration = result.generations[0][0];\n        // TODO: Remove cast after figuring out inheritance\n        return chatGeneration.message;\n    }\n    // eslint-disable-next-line require-yield\n    async *_streamResponseChunks(_messages, _options, _runManager) {\n        throw new Error(\"Not implemented.\");\n    }\n    async *_streamIterator(input, options) {\n        // Subclass check required to avoid double callbacks with default implementation\n        if (this._streamResponseChunks ===\n            BaseChatModel.prototype._streamResponseChunks) {\n            yield this.invoke(input, options);\n        }\n        else {\n            const prompt = BaseChatModel._convertInputToPromptValue(input);\n            const messages = prompt.toChatMessages();\n            const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(options);\n            const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: callOptions,\n                invocation_params: this?.invocationParams(callOptions),\n            };\n            const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), [messages], undefined, undefined, extra);\n            let generationChunk;\n            try {\n                for await (const chunk of this._streamResponseChunks(messages, callOptions, runManagers?.[0])) {\n                    yield chunk.message;\n                    if (!generationChunk) {\n                        generationChunk = chunk;\n                    }\n                    else {\n                        generationChunk = generationChunk.concat(chunk);\n                    }\n                }\n            }\n            catch (err) {\n                await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n                throw err;\n            }\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMEnd({\n                // TODO: Remove cast after figuring out inheritance\n                generations: [[generationChunk]],\n            })));\n        }\n    }\n    /**\n     * Generates chat based on the input messages.\n     * @param messages An array of arrays of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to an LLMResult.\n     */\n    async generate(messages, options, callbacks) {\n        // parse call options\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else {\n            parsedOptions = options;\n        }\n        const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptions(parsedOptions);\n        // create callback manager and start run\n        const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks ?? callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: callOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n        };\n        const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), messages, undefined, undefined, extra);\n        // generate results\n        const results = await Promise.allSettled(messages.map((messageList, i) => this._generate(messageList, { ...callOptions, promptIndex: i }, runManagers?.[i])));\n        // handle results\n        const generations = [];\n        const llmOutputs = [];\n        await Promise.all(results.map(async (pResult, i) => {\n            if (pResult.status === \"fulfilled\") {\n                const result = pResult.value;\n                generations[i] = result.generations;\n                llmOutputs[i] = result.llmOutput;\n                return runManagers?.[i]?.handleLLMEnd({\n                    generations: [result.generations],\n                    llmOutput: result.llmOutput,\n                });\n            }\n            else {\n                // status === \"rejected\"\n                await runManagers?.[i]?.handleLLMError(pResult.reason);\n                return Promise.reject(pResult.reason);\n            }\n        }));\n        // create combined output\n        const output = {\n            generations,\n            llmOutput: llmOutputs.length\n                ? this._combineLLMOutput?.(...llmOutputs)\n                : undefined,\n        };\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams(_options) {\n        return {};\n    }\n    _modelType() {\n        return \"base_chat_model\";\n    }\n    /**\n     * Generates a prompt based on the input prompt values.\n     * @param promptValues An array of BasePromptValue instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to an LLMResult.\n     */\n    async generatePrompt(promptValues, options, callbacks) {\n        const promptMessages = promptValues.map((promptValue) => promptValue.toChatMessages());\n        return this.generate(promptMessages, options, callbacks);\n    }\n    /**\n     * Makes a single call to the chat model.\n     * @param messages An array of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async call(messages, options, callbacks) {\n        const result = await this.generate([messages], options, callbacks);\n        const generations = result.generations;\n        return generations[0][0].message;\n    }\n    /**\n     * Makes a single call to the chat model with a prompt value.\n     * @param promptValue The value of the prompt.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async callPrompt(promptValue, options, callbacks) {\n        const promptMessages = promptValue.toChatMessages();\n        return this.call(promptMessages, options, callbacks);\n    }\n    /**\n     * Predicts the next message based on the input messages.\n     * @param messages An array of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async predictMessages(messages, options, callbacks) {\n        return this.call(messages, options, callbacks);\n    }\n    /**\n     * Predicts the next message based on a text input.\n     * @param text The text input.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a string.\n     */\n    async predict(text, options, callbacks) {\n        const message = new HumanMessage(text);\n        const result = await this.call([message], options, callbacks);\n        return result.content;\n    }\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport class SimpleChatModel extends BaseChatModel {\n    async _generate(messages, options, runManager) {\n        const text = await this._call(messages, options, runManager);\n        const message = new AIMessage(text);\n        return {\n            generations: [\n                {\n                    text: message.content,\n                    message,\n                },\n            ],\n        };\n    }\n}\n"],"mappings":"AAAA,SAASA,SAAS,EAAEC,YAAY,EAAEC,OAAO,QAAS,oBAAoB;AACtE,SAASC,iBAAiB,QAAS,2BAA2B;AAC9D,SAASC,eAAe,QAAS,yBAAyB;AAC1D;AACA;AACA;AACA;AACA;AACA,OAAO,SAASC,mCAAmCA,CAAA,EAAG;EAClD,MAAMC,WAAW,GAAG,IAAIC,WAAW,CAAC,CAAC;EACrC,OAAO,IAAIC,eAAe,CAAC;IACvBC,SAASA,CAACC,KAAK,EAAEC,UAAU,EAAE;MACzBA,UAAU,CAACC,OAAO,CAACN,WAAW,CAACO,MAAM,CAACH,KAAK,CAACI,OAAO,CAAC,CAAC;IACzD;EACJ,CAAC,CAAC;AACN;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,aAAa,SAASZ,iBAAiB,CAAC;EACjDa,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACbC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC,WAAW,EAAE,aAAa,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;IACvD,CAAC,CAAC;EACN;EACAC,sCAAsCA,CAACC,OAAO,EAAE;IAC5C,MAAM,CAACC,cAAc,EAAEC,WAAW,CAAC,GAAG,KAAK,CAACH,sCAAsC,CAACC,OAAO,CAAC;IAC3F,IAAIE,WAAW,EAAEC,OAAO,IAAI,CAACD,WAAW,CAACE,MAAM,EAAE;MAC7CF,WAAW,CAACE,MAAM,GAAGC,WAAW,CAACF,OAAO,CAACD,WAAW,CAACC,OAAO,CAAC;IACjE;IACA,OAAO,CAACF,cAAc,EAAEC,WAAW,CAAC;EACxC;EACA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMI,MAAMA,CAACC,KAAK,EAAEP,OAAO,EAAE;IACzB,MAAMQ,WAAW,GAAGnB,aAAa,CAACoB,0BAA0B,CAACF,KAAK,CAAC;IACnE,MAAMG,MAAM,GAAG,MAAM,IAAI,CAACC,cAAc,CAAC,CAACH,WAAW,CAAC,EAAER,OAAO,EAAEA,OAAO,EAAEY,SAAS,CAAC;IACpF,MAAMC,cAAc,GAAGH,MAAM,CAACI,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC/C;IACA,OAAOD,cAAc,CAACE,OAAO;EACjC;EACA;EACA,OAAOC,qBAAqBA,CAACC,SAAS,EAAEC,QAAQ,EAAEC,WAAW,EAAE;IAC3D,MAAM,IAAIC,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACA,OAAOC,eAAeA,CAACd,KAAK,EAAEP,OAAO,EAAE;IACnC;IACA,IAAI,IAAI,CAACgB,qBAAqB,KAC1B3B,aAAa,CAACiC,SAAS,CAACN,qBAAqB,EAAE;MAC/C,MAAM,IAAI,CAACV,MAAM,CAACC,KAAK,EAAEP,OAAO,CAAC;IACrC,CAAC,MACI;MACD,MAAMuB,MAAM,GAAGlC,aAAa,CAACoB,0BAA0B,CAACF,KAAK,CAAC;MAC9D,MAAMiB,QAAQ,GAAGD,MAAM,CAACE,cAAc,CAAC,CAAC;MACxC,MAAM,CAACxB,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACH,sCAAsC,CAACC,OAAO,CAAC;MAC1F,MAAM0B,gBAAgB,GAAG,MAAMhD,eAAe,CAACiD,SAAS,CAAC1B,cAAc,CAACW,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEX,cAAc,CAAC2B,IAAI,EAAE,IAAI,CAACA,IAAI,EAAE3B,cAAc,CAAC4B,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;QAAEC,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACrM,MAAMC,KAAK,GAAG;QACV/B,OAAO,EAAEE,WAAW;QACpB8B,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAAC/B,WAAW;MACzD,CAAC;MACD,MAAMgC,WAAW,GAAG,MAAMR,gBAAgB,EAAES,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAE,CAACZ,QAAQ,CAAC,EAAEa,SAAS,EAAEA,SAAS,EAAEN,KAAK,CAAC;MACxH,IAAIO,eAAe;MACnB,IAAI;QACA,WAAW,MAAMtD,KAAK,IAAI,IAAI,CAACgC,qBAAqB,CAACQ,QAAQ,EAAEtB,WAAW,EAAEgC,WAAW,GAAG,CAAC,CAAC,CAAC,EAAE;UAC3F,MAAMlD,KAAK,CAAC+B,OAAO;UACnB,IAAI,CAACuB,eAAe,EAAE;YAClBA,eAAe,GAAGtD,KAAK;UAC3B,CAAC,MACI;YACDsD,eAAe,GAAGA,eAAe,CAACC,MAAM,CAACvD,KAAK,CAAC;UACnD;QACJ;MACJ,CAAC,CACD,OAAOwD,GAAG,EAAE;QACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACR,WAAW,IAAI,EAAE,EAAES,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;QAC3F,MAAMA,GAAG;MACb;MACA,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACR,WAAW,IAAI,EAAE,EAAES,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEE,YAAY,CAAC;QAC/E;QACAhC,WAAW,EAAE,CAAC,CAACwB,eAAe,CAAC;MACnC,CAAC,CAAC,CAAC,CAAC;IACR;EACJ;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMS,QAAQA,CAACvB,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IACzC;IACA,IAAIoC,aAAa;IACjB,IAAIC,KAAK,CAACC,OAAO,CAAClD,OAAO,CAAC,EAAE;MACxBgD,aAAa,GAAG;QAAEG,IAAI,EAAEnD;MAAQ,CAAC;IACrC,CAAC,MACI;MACDgD,aAAa,GAAGhD,OAAO;IAC3B;IACA,MAAM,CAACC,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACH,sCAAsC,CAACiD,aAAa,CAAC;IAChG;IACA,MAAMtB,gBAAgB,GAAG,MAAMhD,eAAe,CAACiD,SAAS,CAAC1B,cAAc,CAACW,SAAS,IAAIA,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEX,cAAc,CAAC2B,IAAI,EAAE,IAAI,CAACA,IAAI,EAAE3B,cAAc,CAAC4B,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IAClN,MAAMC,KAAK,GAAG;MACV/B,OAAO,EAAEE,WAAW;MACpB8B,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAACe,aAAa;IAC3D,CAAC;IACD,MAAMd,WAAW,GAAG,MAAMR,gBAAgB,EAAES,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEZ,QAAQ,EAAEa,SAAS,EAAEA,SAAS,EAAEN,KAAK,CAAC;IACtH;IACA,MAAMqB,OAAO,GAAG,MAAMX,OAAO,CAACY,UAAU,CAAC7B,QAAQ,CAACmB,GAAG,CAAC,CAACW,WAAW,EAAEC,CAAC,KAAK,IAAI,CAACC,SAAS,CAACF,WAAW,EAAE;MAAE,GAAGpD,WAAW;MAAEuD,WAAW,EAAEF;IAAE,CAAC,EAAErB,WAAW,GAAGqB,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7J;IACA,MAAMzC,WAAW,GAAG,EAAE;IACtB,MAAM4C,UAAU,GAAG,EAAE;IACrB,MAAMjB,OAAO,CAACC,GAAG,CAACU,OAAO,CAACT,GAAG,CAAC,OAAOgB,OAAO,EAAEJ,CAAC,KAAK;MAChD,IAAII,OAAO,CAACC,MAAM,KAAK,WAAW,EAAE;QAChC,MAAMlD,MAAM,GAAGiD,OAAO,CAAC9D,KAAK;QAC5BiB,WAAW,CAACyC,CAAC,CAAC,GAAG7C,MAAM,CAACI,WAAW;QACnC4C,UAAU,CAACH,CAAC,CAAC,GAAG7C,MAAM,CAACmD,SAAS;QAChC,OAAO3B,WAAW,GAAGqB,CAAC,CAAC,EAAET,YAAY,CAAC;UAClChC,WAAW,EAAE,CAACJ,MAAM,CAACI,WAAW,CAAC;UACjC+C,SAAS,EAAEnD,MAAM,CAACmD;QACtB,CAAC,CAAC;MACN,CAAC,MACI;QACD;QACA,MAAM3B,WAAW,GAAGqB,CAAC,CAAC,EAAEV,cAAc,CAACc,OAAO,CAACG,MAAM,CAAC;QACtD,OAAOrB,OAAO,CAACsB,MAAM,CAACJ,OAAO,CAACG,MAAM,CAAC;MACzC;IACJ,CAAC,CAAC,CAAC;IACH;IACA,MAAME,MAAM,GAAG;MACXlD,WAAW;MACX+C,SAAS,EAAEH,UAAU,CAACO,MAAM,GACtB,IAAI,CAACC,iBAAiB,GAAG,GAAGR,UAAU,CAAC,GACvCrB;IACV,CAAC;IACD7C,MAAM,CAACC,cAAc,CAACuE,MAAM,EAAExF,OAAO,EAAE;MACnCqB,KAAK,EAAEqC,WAAW,GACZ;QAAEiC,MAAM,EAAEjC,WAAW,EAAES,GAAG,CAAEyB,OAAO,IAAKA,OAAO,CAACC,KAAK;MAAE,CAAC,GACxDhC,SAAS;MACf1C,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOqE,MAAM;EACjB;EACA;AACJ;AACA;EACI;EACA/B,gBAAgBA,CAACf,QAAQ,EAAE;IACvB,OAAO,CAAC,CAAC;EACb;EACAoD,UAAUA,CAAA,EAAG;IACT,OAAO,iBAAiB;EAC5B;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAM3D,cAAcA,CAAC4D,YAAY,EAAEvE,OAAO,EAAEY,SAAS,EAAE;IACnD,MAAM4D,cAAc,GAAGD,YAAY,CAAC5B,GAAG,CAAEnC,WAAW,IAAKA,WAAW,CAACiB,cAAc,CAAC,CAAC,CAAC;IACtF,OAAO,IAAI,CAACsB,QAAQ,CAACyB,cAAc,EAAExE,OAAO,EAAEY,SAAS,CAAC;EAC5D;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAM6D,IAAIA,CAACjD,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IACrC,MAAMF,MAAM,GAAG,MAAM,IAAI,CAACqC,QAAQ,CAAC,CAACvB,QAAQ,CAAC,EAAExB,OAAO,EAAEY,SAAS,CAAC;IAClE,MAAME,WAAW,GAAGJ,MAAM,CAACI,WAAW;IACtC,OAAOA,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,OAAO;EACpC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAM2D,UAAUA,CAAClE,WAAW,EAAER,OAAO,EAAEY,SAAS,EAAE;IAC9C,MAAM4D,cAAc,GAAGhE,WAAW,CAACiB,cAAc,CAAC,CAAC;IACnD,OAAO,IAAI,CAACgD,IAAI,CAACD,cAAc,EAAExE,OAAO,EAAEY,SAAS,CAAC;EACxD;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAM+D,eAAeA,CAACnD,QAAQ,EAAExB,OAAO,EAAEY,SAAS,EAAE;IAChD,OAAO,IAAI,CAAC6D,IAAI,CAACjD,QAAQ,EAAExB,OAAO,EAAEY,SAAS,CAAC;EAClD;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMgE,OAAOA,CAACC,IAAI,EAAE7E,OAAO,EAAEY,SAAS,EAAE;IACpC,MAAMG,OAAO,GAAG,IAAIxC,YAAY,CAACsG,IAAI,CAAC;IACtC,MAAMnE,MAAM,GAAG,MAAM,IAAI,CAAC+D,IAAI,CAAC,CAAC1D,OAAO,CAAC,EAAEf,OAAO,EAAEY,SAAS,CAAC;IAC7D,OAAOF,MAAM,CAACtB,OAAO;EACzB;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM0F,eAAe,SAASzF,aAAa,CAAC;EAC/C,MAAMmE,SAASA,CAAChC,QAAQ,EAAExB,OAAO,EAAE4C,UAAU,EAAE;IAC3C,MAAMiC,IAAI,GAAG,MAAM,IAAI,CAACE,KAAK,CAACvD,QAAQ,EAAExB,OAAO,EAAE4C,UAAU,CAAC;IAC5D,MAAM7B,OAAO,GAAG,IAAIzC,SAAS,CAACuG,IAAI,CAAC;IACnC,OAAO;MACH/D,WAAW,EAAE,CACT;QACI+D,IAAI,EAAE9D,OAAO,CAAC3B,OAAO;QACrB2B;MACJ,CAAC;IAET,CAAC;EACL;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}