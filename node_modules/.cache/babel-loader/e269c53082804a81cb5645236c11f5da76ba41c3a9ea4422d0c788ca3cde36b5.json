{"ast":null,"code":"import { OpenAIChat } from \"langchain/llms/openai\";\nimport { ChatPromptTemplate } from \"langchain/prompts\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nconst openai = new OpenAIChat({\n  apiKey: process.env.OPENAI_API_KEY\n});\nexport const getAIMessage = async userQuery => {\n  const message = {\n    role: \"assistant\",\n    content: \"Connect your backend here....\"\n  };\n  return message;\n};","map":{"version":3,"names":["OpenAIChat","ChatPromptTemplate","BufferMemory","LLMChain","openai","apiKey","process","env","OPENAI_API_KEY","getAIMessage","userQuery","message","role","content"],"sources":["/Users/mayamagavi/instalily/case-study/src/api/api.js"],"sourcesContent":["import { OpenAIChat } from \"langchain/llms/openai\";\nimport { ChatPromptTemplate } from \"langchain/prompts\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\n\nconst openai = new OpenAIChat({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nexport const getAIMessage = async (userQuery) => {\n\n\n\n  const message = \n    {\n      role: \"assistant\",\n      content: \"Connect your backend here....\"\n    }\n\n  return message;\n};"],"mappings":"AAAA,SAASA,UAAU,QAAQ,uBAAuB;AAClD,SAASC,kBAAkB,QAAQ,mBAAmB;AACtD,SAASC,YAAY,QAAQ,kBAAkB;AAC/C,SAASC,QAAQ,QAAQ,kBAAkB;AAE3C,MAAMC,MAAM,GAAG,IAAIJ,UAAU,CAAC;EAC5BK,MAAM,EAAEC,OAAO,CAACC,GAAG,CAACC;AACtB,CAAC,CAAC;AAEF,OAAO,MAAMC,YAAY,GAAG,MAAOC,SAAS,IAAK;EAI/C,MAAMC,OAAO,GACX;IACEC,IAAI,EAAE,WAAW;IACjBC,OAAO,EAAE;EACX,CAAC;EAEH,OAAOF,OAAO;AAChB,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}