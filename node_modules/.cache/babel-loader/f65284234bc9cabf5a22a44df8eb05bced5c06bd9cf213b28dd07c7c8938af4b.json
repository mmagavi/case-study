{"ast":null,"code":"import { encodingForModel } from \"../util/tiktoken.js\";\n// https://www.npmjs.com/package/js-tiktoken\nexport const getModelNameForTiktoken = modelName => {\n  if (modelName.startsWith(\"gpt-3.5-turbo-16k\")) {\n    return \"gpt-3.5-turbo-16k\";\n  }\n  if (modelName.startsWith(\"gpt-3.5-turbo-\")) {\n    return \"gpt-3.5-turbo\";\n  }\n  if (modelName.startsWith(\"gpt-4-32k\")) {\n    return \"gpt-4-32k\";\n  }\n  if (modelName.startsWith(\"gpt-4-\")) {\n    return \"gpt-4\";\n  }\n  return modelName;\n};\nexport const getEmbeddingContextSize = modelName => {\n  switch (modelName) {\n    case \"text-embedding-ada-002\":\n      return 8191;\n    default:\n      return 2046;\n  }\n};\nexport const getModelContextSize = modelName => {\n  switch (getModelNameForTiktoken(modelName)) {\n    case \"gpt-3.5-turbo-16k\":\n      return 16384;\n    case \"gpt-3.5-turbo\":\n      return 4096;\n    case \"gpt-4-32k\":\n      return 32768;\n    case \"gpt-4\":\n      return 8192;\n    case \"text-davinci-003\":\n      return 4097;\n    case \"text-curie-001\":\n      return 2048;\n    case \"text-babbage-001\":\n      return 2048;\n    case \"text-ada-001\":\n      return 2048;\n    case \"code-davinci-002\":\n      return 8000;\n    case \"code-cushman-001\":\n      return 2048;\n    default:\n      return 4097;\n  }\n};\nexport const calculateMaxTokens = async ({\n  prompt,\n  modelName\n}) => {\n  let numTokens;\n  try {\n    numTokens = (await encodingForModel(modelName)).encode(prompt).length;\n  } catch (error) {\n    console.warn(\"Failed to calculate number of tokens, falling back to approximate count\");\n    // fallback to approximate calculation if tiktoken is not available\n    numTokens = Math.ceil(prompt.length / 4);\n  }\n  const maxTokens = getModelContextSize(modelName);\n  return maxTokens - numTokens;\n};","map":{"version":3,"names":["encodingForModel","getModelNameForTiktoken","modelName","startsWith","getEmbeddingContextSize","getModelContextSize","calculateMaxTokens","prompt","numTokens","encode","length","error","console","warn","Math","ceil","maxTokens"],"sources":["/Users/mayamagavi/instalily/case-study/node_modules/langchain/dist/base_language/count_tokens.js"],"sourcesContent":["import { encodingForModel } from \"../util/tiktoken.js\";\n// https://www.npmjs.com/package/js-tiktoken\nexport const getModelNameForTiktoken = (modelName) => {\n    if (modelName.startsWith(\"gpt-3.5-turbo-16k\")) {\n        return \"gpt-3.5-turbo-16k\";\n    }\n    if (modelName.startsWith(\"gpt-3.5-turbo-\")) {\n        return \"gpt-3.5-turbo\";\n    }\n    if (modelName.startsWith(\"gpt-4-32k\")) {\n        return \"gpt-4-32k\";\n    }\n    if (modelName.startsWith(\"gpt-4-\")) {\n        return \"gpt-4\";\n    }\n    return modelName;\n};\nexport const getEmbeddingContextSize = (modelName) => {\n    switch (modelName) {\n        case \"text-embedding-ada-002\":\n            return 8191;\n        default:\n            return 2046;\n    }\n};\nexport const getModelContextSize = (modelName) => {\n    switch (getModelNameForTiktoken(modelName)) {\n        case \"gpt-3.5-turbo-16k\":\n            return 16384;\n        case \"gpt-3.5-turbo\":\n            return 4096;\n        case \"gpt-4-32k\":\n            return 32768;\n        case \"gpt-4\":\n            return 8192;\n        case \"text-davinci-003\":\n            return 4097;\n        case \"text-curie-001\":\n            return 2048;\n        case \"text-babbage-001\":\n            return 2048;\n        case \"text-ada-001\":\n            return 2048;\n        case \"code-davinci-002\":\n            return 8000;\n        case \"code-cushman-001\":\n            return 2048;\n        default:\n            return 4097;\n    }\n};\nexport const calculateMaxTokens = async ({ prompt, modelName, }) => {\n    let numTokens;\n    try {\n        numTokens = (await encodingForModel(modelName)).encode(prompt).length;\n    }\n    catch (error) {\n        console.warn(\"Failed to calculate number of tokens, falling back to approximate count\");\n        // fallback to approximate calculation if tiktoken is not available\n        numTokens = Math.ceil(prompt.length / 4);\n    }\n    const maxTokens = getModelContextSize(modelName);\n    return maxTokens - numTokens;\n};\n"],"mappings":"AAAA,SAASA,gBAAgB,QAAQ,qBAAqB;AACtD;AACA,OAAO,MAAMC,uBAAuB,GAAIC,SAAS,IAAK;EAClD,IAAIA,SAAS,CAACC,UAAU,CAAC,mBAAmB,CAAC,EAAE;IAC3C,OAAO,mBAAmB;EAC9B;EACA,IAAID,SAAS,CAACC,UAAU,CAAC,gBAAgB,CAAC,EAAE;IACxC,OAAO,eAAe;EAC1B;EACA,IAAID,SAAS,CAACC,UAAU,CAAC,WAAW,CAAC,EAAE;IACnC,OAAO,WAAW;EACtB;EACA,IAAID,SAAS,CAACC,UAAU,CAAC,QAAQ,CAAC,EAAE;IAChC,OAAO,OAAO;EAClB;EACA,OAAOD,SAAS;AACpB,CAAC;AACD,OAAO,MAAME,uBAAuB,GAAIF,SAAS,IAAK;EAClD,QAAQA,SAAS;IACb,KAAK,wBAAwB;MACzB,OAAO,IAAI;IACf;MACI,OAAO,IAAI;EACnB;AACJ,CAAC;AACD,OAAO,MAAMG,mBAAmB,GAAIH,SAAS,IAAK;EAC9C,QAAQD,uBAAuB,CAACC,SAAS,CAAC;IACtC,KAAK,mBAAmB;MACpB,OAAO,KAAK;IAChB,KAAK,eAAe;MAChB,OAAO,IAAI;IACf,KAAK,WAAW;MACZ,OAAO,KAAK;IAChB,KAAK,OAAO;MACR,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,gBAAgB;MACjB,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,cAAc;MACf,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf,KAAK,kBAAkB;MACnB,OAAO,IAAI;IACf;MACI,OAAO,IAAI;EACnB;AACJ,CAAC;AACD,OAAO,MAAMI,kBAAkB,GAAG,MAAAA,CAAO;EAAEC,MAAM;EAAEL;AAAW,CAAC,KAAK;EAChE,IAAIM,SAAS;EACb,IAAI;IACAA,SAAS,GAAG,CAAC,MAAMR,gBAAgB,CAACE,SAAS,CAAC,EAAEO,MAAM,CAACF,MAAM,CAAC,CAACG,MAAM;EACzE,CAAC,CACD,OAAOC,KAAK,EAAE;IACVC,OAAO,CAACC,IAAI,CAAC,yEAAyE,CAAC;IACvF;IACAL,SAAS,GAAGM,IAAI,CAACC,IAAI,CAACR,MAAM,CAACG,MAAM,GAAG,CAAC,CAAC;EAC5C;EACA,MAAMM,SAAS,GAAGX,mBAAmB,CAACH,SAAS,CAAC;EAChD,OAAOc,SAAS,GAAGR,SAAS;AAChC,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}